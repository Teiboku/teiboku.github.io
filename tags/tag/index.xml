<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tag on Kannsou</title><link>https://teiboku.github.io/tags/tag/</link><description>Recent content in Tag on Kannsou</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 15 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://teiboku.github.io/tags/tag/index.xml" rel="self" type="application/rss+xml"/><item><title>A model release pipeline</title><link>https://teiboku.github.io/posts/1736923185327-a-model-release-pipeline/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://teiboku.github.io/posts/1736923185327-a-model-release-pipeline/</guid><description>&lt;p>an example to get you started&lt;/p>
&lt;h1 class="relative group">This is a heading
&lt;div id="this-is-a-heading" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#this-is-a-heading" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h1>
&lt;h2 class="relative group">This is a subheading
&lt;div id="this-is-a-subheading" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#this-is-a-subheading" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;h3 class="relative group">This is a subsubheading
&lt;div id="this-is-a-subsubheading" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#this-is-a-subsubheading" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h3>
&lt;h4 class="relative group">This is a subsubsubheading
&lt;div id="this-is-a-subsubsubheading" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#this-is-a-subsubsubheading" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h4>
&lt;p>This is a paragraph with &lt;strong>bold&lt;/strong> and &lt;em>italic&lt;/em> text.
Check more at &lt;a href="https://blowfish.page/" target="_blank">Blowfish documentation&lt;/a>
undefined&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://teiboku.github.io/posts/1736923185327-a-model-release-pipeline/featured.png"/></item><item><title>Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes</title><link>https://teiboku.github.io/posts/ehz/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://teiboku.github.io/posts/ehz/</guid><description>&lt;h2 class="relative group">Abstract
&lt;div id="abstract" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#abstract" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>DSLR cameras can achieve various zoom levels by changing the lens distance or swapping lens types. However, due to space constraints, these techniques are not feasible on mobile devices. Most smartphones adopt a &lt;strong>hybrid zoom system&lt;/strong>: typically using a wide-angle (W) camera for low zoom levels and a telephoto (T) camera for high zoom levels. To simulate zoom levels between W and T, these systems crop and digitally enlarge images from W, resulting in significant detail loss. In this paper, we propose an efficient hybrid zoom super-resolution system for mobile devices. This system captures &lt;strong>synchronized W and T image pairs&lt;/strong> and &lt;strong>utilizes a machine learning model to align and transfer details from T to W&lt;/strong>. We further develop an &lt;strong>adaptive blending method&lt;/strong> that can handle &lt;strong>depth of field mismatches, scene occlusions, motion uncertainties, and alignment errors&lt;/strong>. To minimize domain differences, we design a dual smartphone camera setup to capture real scene inputs and real label data for supervised training. In extensive evaluations on real scenes, our method can generate 12-megapixel images on mobile platforms in 500 milliseconds, achieving TA.&lt;/p></description></item><item><title>Two Phase Neural Sparse Search On AWS OpenSearch</title><link>https://teiboku.github.io/projects/1736922318122-neural-sparse-search-on-aws-opensearch/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://teiboku.github.io/projects/1736922318122-neural-sparse-search-on-aws-opensearch/</guid><description>&lt;h2 class="relative group">Some link for your reference
&lt;div id="some-link-for-your-reference" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#some-link-for-your-reference" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>&lt;a href="https://github.com/opensearch-project/neural-search/issues/646" target="_blank">RFC by me&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/opensearch-project/neural-search/pull/695/files" target="_blank">PR by me&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://opensearch.org/blog/Introducing-a-neural-sparse-two-phase-algorithm/" target="_blank">Blog post by me&lt;/a>&lt;/p>
&lt;h2 class="relative group">Introduction
&lt;div id="introduction" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#introduction" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>Neural sparse search is an efficient method for semantic retrieval introduced in OpenSearch 2.11. It uses semantic techniques to interpret queries, handling terms that traditional search may miss. While dense models find similar results, they can overlook exact matches. Neural sparse search uses sparse representations to capture both semantic similarities and specific terms, improving result explanation and presentation by providing a more comprehensive retrieval solution.&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://teiboku.github.io/projects/1736922318122-neural-sparse-search-on-aws-opensearch/featured.png"/></item><item><title>D5 Render-Rhino Sync Plugin</title><link>https://teiboku.github.io/projects/1735963425218-d5-render-rhino-sync-plugin/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://teiboku.github.io/projects/1735963425218-d5-render-rhino-sync-plugin/</guid><description>&lt;h2 class="relative group">D5 Render
&lt;div id="d5-render" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#d5-render" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>D5 Render is a cutting-edge, real-time 3D rendering software designed to provide architects, designers, and visualization professionals with a seamless and efficient tool for creating photorealistic visuals. Its intuitive interface, advanced rendering engine, and versatile features make it an excellent choice for professionals seeking high-quality results with minimal learning curve.&lt;/p>
&lt;h2 class="relative group">User Case
&lt;div id="user-case" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#user-case" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>Most users typically work with various 3D modeling software to create and save their 3D model data, which they then import into D5 Render for real-time rendering to observe the realistic effects of lighting.
In the architecture industry, frequent modifications to objects are often required, such as removing a landscape element or resizing certain features. This creates a significant pain point for users, as they need to constantly switch between the rendering software and the 3D modeling software.&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://teiboku.github.io/projects/1735963425218-d5-render-rhino-sync-plugin/featured.png"/></item></channel></rss>