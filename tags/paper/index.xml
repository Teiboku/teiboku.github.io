<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper on Kannsou</title><link>https://teiboku.github.io/tags/paper/</link><description>Recent content in Paper on Kannsou</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 15 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://teiboku.github.io/tags/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes</title><link>https://teiboku.github.io/posts/ehz/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://teiboku.github.io/posts/ehz/</guid><description>&lt;h2 class="relative group">Abstract
&lt;div id="abstract" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#abstract" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>DSLR cameras can achieve various zoom levels by changing the lens distance or swapping lens types. However, due to space constraints, these techniques are not feasible on mobile devices. Most smartphones adopt a &lt;strong>hybrid zoom system&lt;/strong>: typically using a wide-angle (W) camera for low zoom levels and a telephoto (T) camera for high zoom levels. To simulate zoom levels between W and T, these systems crop and digitally enlarge images from W, resulting in significant detail loss. In this paper, we propose an efficient hybrid zoom super-resolution system for mobile devices. This system captures &lt;strong>synchronized W and T image pairs&lt;/strong> and &lt;strong>utilizes a machine learning model to align and transfer details from T to W&lt;/strong>. We further develop an &lt;strong>adaptive blending method&lt;/strong> that can handle &lt;strong>depth of field mismatches, scene occlusions, motion uncertainties, and alignment errors&lt;/strong>. To minimize domain differences, we design a dual smartphone camera setup to capture real scene inputs and real label data for supervised training. In extensive evaluations on real scenes, our method can generate 12-megapixel images on mobile platforms in 500 milliseconds, achieving TA.&lt;/p></description></item></channel></rss>