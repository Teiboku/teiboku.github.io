<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes &#183; Kannsou</title>
<meta name=title content="Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes &#183; Kannsou"><meta name=description content="a description"><meta name=keywords content="example,tag,paper,"><link rel=canonical href=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/><link type=text/css rel=stylesheet href=/css/main.bundle.min.e27868ab1485f7ed7b06b122b4980bd38b19526eb8f7de885181204d28f04a0c47e9c334eff19a06c0278eb2ff8415b983a5d0fb80fd6b5680c926457cc61c57.css integrity="sha512-4nhoqxSF9+17BrEitJgL04sZUm64996IUYEgTSjwSgxH6cM07/GaBsAnjrL/hBW5g6XQ+4D9a1aAySZFfMYcVw=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.a2d78d78672e549fbfc972ece871725b5478ba0b65708dda20cb97ab80a865eae6d247e1b05a4aec6ebbf78647ec3233bad8b2609ed98eee53cd58aa17128bc7.js integrity="sha512-oteNeGcuVJ+/yXLs6HFyW1R4ugtlcI3aIMuXq4CoZerm0kfhsFpK7G6794ZH7DIzutiyYJ7Zju5TzViqFxKLxw==" data-copy data-copied></script><script src=/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/"><meta property="og:site_name" content="Kannsou"><meta property="og:title" content="Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes"><meta property="og:description" content="a description"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-15T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-15T00:00:00+00:00"><meta property="article:tag" content="Example"><meta property="article:tag" content="Tag"><meta property="article:tag" content="Paper"><meta name=twitter:card content="summary"><meta name=twitter:title content="Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes"><meta name=twitter:description content="a description"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes","headline":"Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes","description":"a description","abstract":"\u003ch2 class=\u0022relative group\u0022\u003eAbstract \n    \u003cdiv id=\u0022abstract\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022\n            style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#abstract\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h2\u003e\n\u003cp\u003eDSLR 相机可以通过改变镜头距离或更换镜头类型实现多种变焦级别。然而，由于空间限制，这些技术在智能手机设备上不可行。大多数手机采用\u003cstrong\u003e混合变焦系统\u003c\/strong\u003e：通常使用低变焦级别的广角（W）摄像头和高变焦级别的长焦（T）摄像头。\n为了模拟介于 W 和 T 之间的变焦级别，这些系统会对来自 W 的图像进行裁剪和数字放大，导致细节损失显著。在本文中，我们提出了一种高效的移动设备混合变焦超分辨率系统。该系统捕获\u003cstrong\u003e同步的 W 和 T 图像对\u003c\/strong\u003e，并\u003cstrong\u003e利用机器学习模型将 T 的细节对齐并传递到 W\u003c\/strong\u003e。我们进一步开发了一种\u003cstrong\u003e自适应混合方法\u003c\/strong\u003e，该方法可以处理\u003cstrong\u003e景深不匹配、场景遮挡、流动不确定性以及对齐误差\u003c\/strong\u003e。为了尽量减少域差异，我们设计了一个双手机摄像头装置，用于捕捉真实场景输入和用于监督训练的真实标签数据。在真实场景的广泛评估中，我们的方法可以在移动平台上生成 12 百万像素图像，耗时 500 毫秒，并达到了TA.\u003c\/p\u003e\n\n\n\u003ch2 class=\u0022relative group\u0022\u003eKey word \n    \u003cdiv id=\u0022key-word\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022\n            style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#key-word\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h2\u003e\n\u003cp\u003ehybrid zoom, dual camera fusion, deep neural networks\u003c\/p\u003e\n\n\n\u003ch2 class=\u0022relative group\u0022\u003eKey Takeaways \n    \u003cdiv id=\u0022key-takeaways\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022\n            style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#key-takeaways\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h2\u003e\n\n\n\u003ch3 class=\u0022relative group\u0022\u003eProblem Addressed \n    \u003cdiv id=\u0022problem-addressed\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022\n            style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#problem-addressed\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h3\u003e\n\u003cp\u003e几乎所有的上采样方法（如双线性插值、双三次插值等）都会导致不同程度的画质下降, 因此, 涌现了众多利用高变焦 T 图像作为参考，为低变焦 W 图像添加真实细节的方法. 商业解决方案并未公开, 学术研究上效率较低. 。这些方法通常在移动设备上效率较低，容易受到参考图像缺陷的影响，并可能在训练和推理之间引入域差异。\n对以上问题.提出了一种混合变焦超分辨率（Hybrid Zoom Super-Resolution, HZSR）系统以加以解决。\u003c\/p\u003e","inLanguage":"en","url":"https:\/\/teiboku.github.io\/posts\/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones\/","author":{"@type":"Person","name":"かん そう"},"copyrightYear":"2025","dateCreated":"2025-01-15T00:00:00\u002b00:00","datePublished":"2025-01-15T00:00:00\u002b00:00","dateModified":"2025-01-15T00:00:00\u002b00:00","keywords":["example","tag","paper"],"mainEntityOfPage":"true","wordCount":"765"}]</script><meta name=author content="かん そう"><link href=mailto:bkannsou@gmail.com rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link type=text/css rel=stylesheet href=/lib/katex/katex.min.68e17230ccd917b97b7a2def38a8108918599d8aa4f580bfb8cce5e13d23e4de43dcaba5f9000553cb2c10d0d1300aabfe5c433a3305ebd752609f0762a63e59.css integrity="sha512-aOFyMMzZF7l7ei3vOKgQiRhZnYqk9YC/uMzl4T0j5N5D3Kul+QAFU8ssENDRMAqr/lxDOjMF69dSYJ8HYqY+WQ=="><script defer src=/lib/katex/katex.min.50f14e69d6a8da7128ae3b63974c544ed377c36d096b5e3750f114e84c89d668b9301d9b0ed3248969aa183aa2e3bc4d2c1e73d5dcb7d462890c45a18d424589.js integrity="sha512-UPFOadao2nEorjtjl0xUTtN3w20Ja143UPEU6EyJ1mi5MB2bDtMkiWmqGDqi47xNLB5z1dy31GKJDEWhjUJFiQ=="></script><script defer src=/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" onload=renderMathInElement(document.body)></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Kannsou</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Posts</p></a><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Projects>Projects</p></a><a href=/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="My Resume">Resume</p></a><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Posts</p></a></li><li class=mt-1><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Projects>Projects</p></a></li><li class=mt-1><a href=/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="My Resume">Resume</p></a></li><li class=mt-1><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li></ul></div></label></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>Kannsou</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/>Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-01-15T00:00:00+00:00>15 January 2025</time><span class="px-2 text-primary-500">&#183;</span><span>765 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">4 mins</span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="かん そう" src=/goubao_hu10798311406838057196.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">かん そう</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I&rsquo;m a computer science student at the Waseda University, Tokyo, Japan. I&rsquo;m interested in machine learning, natural language processing, and computer vision.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:bkannsou@gmail.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#key-word>Key word</a></li><li><a href=#key-takeaways>Key Takeaways</a><ul><li><a href=#problem-addressed>Problem Addressed</a></li><li><a href=#main-contributions>Main Contributions</a></li></ul></li><li><a href=#methodology>Methodology</a><ul><li><a href=#approach>Approach</a><ul><li><a href=#适应不完美的参考图像><strong>适应不完美的参考图像</strong></a></li><li><a href=#通过真实场景输入最小化域差异>通过真实场景输入最小化域差异</a></li></ul></li><li><a href=#modelalgorithm>Model/Algorithm</a><ul><li><a href=#图像对齐alignment><strong>图像对齐（Alignment）</strong>：</a></li><li><a href=#图像融合fusion><strong>图像融合（Fusion）</strong>：</a></li><li><a href=#自适应混合adaptive-blending><strong>自适应混合（Adaptive Blending）</strong>：</a></li><li><a href=#最终混合>最终混合</a></li></ul></li><li><a href=#implementation-details>Implementation Details</a></li></ul></li><li><a href=#results-and-evaluation>Results and Evaluation</a><ul><li><a href=#experiments>Experiments:</a></li><li><a href=#results>Results</a></li><li><a href=#comparisons>Comparisons</a></li></ul></li><li><a href=#critical-analysis>Critical Analysis</a><ul><li><a href=#strengths>Strengths</a></li><li><a href=#weaknesses>Weaknesses</a></li><li><a href=#insights>Insights</a></li></ul></li><li><a href=#broader-impact-and-future-work>Broader Impact and Future Work</a><ul><li><a href=#broader-implications>Broader Implications</a></li><li><a href=#open-questions>Open Questions</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#personal-reflection>Personal Reflection</a><ul><li><a href=#relevance-to-my-work>Relevance to My Work</a></li><li><a href=#practical-applications>Practical Applications</a></li><li><a href=#key-learnings>Key Learnings</a></li></ul></li><li><a href=#related-work-and-context>Related Work and Context</a><ul><li><a href=#prior-work>Prior Work</a></li><li><a href=#historical-context>Historical Context</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#key-word>Key word</a></li><li><a href=#key-takeaways>Key Takeaways</a><ul><li><a href=#problem-addressed>Problem Addressed</a></li><li><a href=#main-contributions>Main Contributions</a></li></ul></li><li><a href=#methodology>Methodology</a><ul><li><a href=#approach>Approach</a><ul><li><a href=#适应不完美的参考图像><strong>适应不完美的参考图像</strong></a></li><li><a href=#通过真实场景输入最小化域差异>通过真实场景输入最小化域差异</a></li></ul></li><li><a href=#modelalgorithm>Model/Algorithm</a><ul><li><a href=#图像对齐alignment><strong>图像对齐（Alignment）</strong>：</a></li><li><a href=#图像融合fusion><strong>图像融合（Fusion）</strong>：</a></li><li><a href=#自适应混合adaptive-blending><strong>自适应混合（Adaptive Blending）</strong>：</a></li><li><a href=#最终混合>最终混合</a></li></ul></li><li><a href=#implementation-details>Implementation Details</a></li></ul></li><li><a href=#results-and-evaluation>Results and Evaluation</a><ul><li><a href=#experiments>Experiments:</a></li><li><a href=#results>Results</a></li><li><a href=#comparisons>Comparisons</a></li></ul></li><li><a href=#critical-analysis>Critical Analysis</a><ul><li><a href=#strengths>Strengths</a></li><li><a href=#weaknesses>Weaknesses</a></li><li><a href=#insights>Insights</a></li></ul></li><li><a href=#broader-impact-and-future-work>Broader Impact and Future Work</a><ul><li><a href=#broader-implications>Broader Implications</a></li><li><a href=#open-questions>Open Questions</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#personal-reflection>Personal Reflection</a><ul><li><a href=#relevance-to-my-work>Relevance to My Work</a></li><li><a href=#practical-applications>Practical Applications</a></li><li><a href=#key-learnings>Key Learnings</a></li></ul></li><li><a href=#related-work-and-context>Related Work and Context</a><ul><li><a href=#prior-work>Prior Work</a></li><li><a href=#historical-context>Historical Context</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><h2 class="relative group">Abstract<div id=abstract class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#abstract aria-label=Anchor>#</a></span></h2><p>DSLR 相机可以通过改变镜头距离或更换镜头类型实现多种变焦级别。然而，由于空间限制，这些技术在智能手机设备上不可行。大多数手机采用<strong>混合变焦系统</strong>：通常使用低变焦级别的广角（W）摄像头和高变焦级别的长焦（T）摄像头。
为了模拟介于 W 和 T 之间的变焦级别，这些系统会对来自 W 的图像进行裁剪和数字放大，导致细节损失显著。在本文中，我们提出了一种高效的移动设备混合变焦超分辨率系统。该系统捕获<strong>同步的 W 和 T 图像对</strong>，并<strong>利用机器学习模型将 T 的细节对齐并传递到 W</strong>。我们进一步开发了一种<strong>自适应混合方法</strong>，该方法可以处理<strong>景深不匹配、场景遮挡、流动不确定性以及对齐误差</strong>。为了尽量减少域差异，我们设计了一个双手机摄像头装置，用于捕捉真实场景输入和用于监督训练的真实标签数据。在真实场景的广泛评估中，我们的方法可以在移动平台上生成 12 百万像素图像，耗时 500 毫秒，并达到了TA.</p><h2 class="relative group">Key word<div id=key-word class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-word aria-label=Anchor>#</a></span></h2><p>hybrid zoom, dual camera fusion, deep neural networks</p><h2 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h2><h3 class="relative group">Problem Addressed<div id=problem-addressed class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#problem-addressed aria-label=Anchor>#</a></span></h3><p>几乎所有的上采样方法（如双线性插值、双三次插值等）都会导致不同程度的画质下降, 因此, 涌现了众多利用高变焦 T 图像作为参考，为低变焦 W 图像添加真实细节的方法. 商业解决方案并未公开, 学术研究上效率较低. 。这些方法通常在移动设备上效率较低，容易受到参考图像缺陷的影响，并可能在训练和推理之间引入域差异。
对以上问题.提出了一种混合变焦超分辨率（Hybrid Zoom Super-Resolution, HZSR）系统以加以解决。</p><h3 class="relative group">Main Contributions<div id=main-contributions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#main-contributions aria-label=Anchor>#</a></span></h3><ul><li><strong>基于机器学习的混合变焦超分辨率（HZSR）系统</strong>，该系统在移动设备上高效运行，并且对真实场景图像中的不完美情况具有较强的鲁棒性（详见第 3 节）。</li><li><strong>一种训练策略</strong>，通过双手机摄像头平台最小化域差异，并避免在参考超分辨率（RefSR）任务中学习到平凡映射（详见第 4 节）。</li><li><strong>一个包含 150 组高分辨率（12MP）的 W 和 T 图像对的高质量同步数据集</strong>，命名为 Hzsr 数据集，将在我们的项目网站上发布，用于未来研究（详见第 5 节）。</li></ul><h2 class="relative group">Methodology<div id=methodology class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#methodology aria-label=Anchor>#</a></span></h2><h3 class="relative group">Approach<div id=approach class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#approach aria-label=Anchor>#</a></span></h3><h4 class="relative group"><strong>适应不完美的参考图像</strong><div id=%E9%80%82%E5%BA%94%E4%B8%8D%E5%AE%8C%E7%BE%8E%E7%9A%84%E5%8F%82%E8%80%83%E5%9B%BE%E5%83%8F class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E9%80%82%E5%BA%94%E4%B8%8D%E5%AE%8C%E7%BE%8E%E7%9A%84%E5%8F%82%E8%80%83%E5%9B%BE%E5%83%8F aria-label=Anchor>#</a></span></h4><p>提出了一种高效的<strong>失焦检测算法</strong>，该算法<strong>基于场景深度和光流之间的相关性来排除失焦区域</strong>。结合<strong>失焦图、对齐误差、光流不确定性和场景遮挡信息</strong>，我们设计了一种<strong>自适应混合机制</strong>，以生成高质量、无伪影的超分辨率结果。</p><h4 class="relative group">通过真实场景输入最小化域差异<div id=%E9%80%9A%E8%BF%87%E7%9C%9F%E5%AE%9E%E5%9C%BA%E6%99%AF%E8%BE%93%E5%85%A5%E6%9C%80%E5%B0%8F%E5%8C%96%E5%9F%9F%E5%B7%AE%E5%BC%82 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E9%80%9A%E8%BF%87%E7%9C%9F%E5%AE%9E%E5%9C%BA%E6%99%AF%E8%BE%93%E5%85%A5%E6%9C%80%E5%B0%8F%E5%8C%96%E5%9F%9F%E5%B7%AE%E5%BC%82 aria-label=Anchor>#</a></span></h4><p>在参考超分辨率（RefSR）任务中，收集完全对齐的 W/T 图像对作为训练数据是非常困难的。因此，以往研究探索了两种可能但不理想的解决方案：</p><ol><li><strong>将参考图像 T 用作训练目标</strong>（如 Wang et al. 2021 和 Zhang et al. 2022a），这种方法可能会传递参考图像的缺陷或导致网络学习恒等映射（identity mapping）。</li><li><strong>通过退化模型从目标图像合成低分辨率输入</strong>（如 Trinidad et al. 2019 和 Zhang et al. 2022a），但此方法在训练和推理阶段之间引入了域差异，进而降低了对真实场景图像的超分辨率效果。
为了避免学习恒等映射并最小化域差异，我们采用了一种设计：在训练过程中，使用安装在摄影平台上的第二部相同型号的智能手机同步捕捉额外的 T 图像作为参考（见图 6）。<figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/fbmyRgT.png alt=povIB4GXtGm5Ljl></figure></li></ol><p>通过这种设计，融合模型在训练和推理阶段均以真实的 W 图像作为输入，从而消除域差异。此外，参考图像和目标图像由不同设备的 T 摄像头捕获，以避免网络学习恒等映射。
与现有双变焦 RefSR 数据集相比，我们的设计具备显著优势：</p><ul><li>一些数据集存在 W 和 T 之间的强时间运动（如 Wang et al. 2021）；</li><li>一些数据集仅限于静态场景（如 Wei et al. 2020）。
我们收集了一个大规模数据集，其中包含高质量的 W/T 同步数据，覆盖了动态场景，包括人像、建筑、风景，以及动态物体运动和夜间场景等具有挑战性的场景。实验表明，我们的方法在现有双变焦 RefSR 数据集和我们的新数据集上均优于当前最先进的方法。</li></ul><h3 class="relative group">Model/Algorithm<div id=modelalgorithm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#modelalgorithm aria-label=Anchor>#</a></span></h3><p>当用户将变焦调整至中等范围（例如 3-5 倍）并按下快门按钮时，系统会捕获一对同步的图像。处理流程如下：</p><h4 class="relative group"><strong>图像对齐（Alignment）</strong>：<div id=%E5%9B%BE%E5%83%8F%E5%AF%B9%E9%BD%90alignment class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E5%9B%BE%E5%83%8F%E5%AF%B9%E9%BD%90alignment aria-label=Anchor>#</a></span></h4><p>首先，通过<strong>关键点匹配进行全局粗对齐</strong>，随后使用<strong>光流执行局部密集对齐</strong>（详见第 3.1 节）。
<strong>粗对齐（Coarse Alignment）：</strong><br>首先，我们对广角摄像头的图像（W）进行裁剪，使其视场角（FOV）与长焦摄像头的图像（T）一致。接着，使用双三次插值将 W 的空间分辨率调整到与 T 匹配（4k×3k）。随后，通过 FAST 特征点匹配算法（Rosten and Drummond 2006）估计全局二维平移向量，并调整裁剪后的 W，得到调整后的图像 IsrcI_{src}Isrc​
<strong>密集对齐（Dense Alignment）：</strong><br>我们使用 PWC-Net估计 $I_{src}​$ 和 $I_{ref}$​ 之间的稠密光流。需要注意的是，W 和 T 之间在 12MP 分辨率下的平均偏移量约为 150 像素，这远大于大多数光流训练数据中的运动幅度。因此，从 12MP 图像估计的光流通常会非常嘈杂。
为了提高精度，我们将 $I_{src}$​ 和 $I_{ref}$​ 下采样到 384×512 的<strong>较小分辨率下预测</strong>光流，然后将光流<strong>上采样到原始分辨率</strong>，并通过双线性重采样将 $I_{ref}$​ 变形得到 $\tilde{I}<em>{ref}$。这种方法在较小尺度下估计的光流更准确且更具鲁棒性。
<strong>优化</strong>
为了适应移动设备的计算资源限制，我们移除了原始 PWC-Net 中的 DenseNet 结构，减少了模型大小 <strong>50%</strong>，延迟 <strong>56%</strong>，以及峰值内存 <strong>63%</strong>。尽管在 Sintel 数据集上的光流端点误差（EPE）增加了 <strong>8%</strong>，但光流的视觉质量依然保持相似。<br>此外，我们通过前向-后向一致性检查（forward-backward consistency check, Alvarez et al. 2007）生成遮挡图 $M</em>{occ}$​，标识在对齐过程中被遮挡的区域。</p><h4 class="relative group"><strong>图像融合（Fusion）</strong>：<div id=%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88fusion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88fusion aria-label=Anchor>#</a></span></h4><p>采用 UNet（Ronneberger et al. 2015）来融合来自广角摄像头（W）的裁剪图像亮度通道与通过变形的长焦摄像头（T）参考图像（详见第 3.2 节）。</p><p>以源图像、变形后的参考图像和遮挡mask作为输入
<strong>融合过程：</strong><br>为了保持 W 图像的颜色一致性，我们仅在亮度空间（Luminance Space）中进行融合。具体实现如下：</p><ol><li><strong>输入准备：</strong><ul><li>将源图像 $I_{src}$​ 转换为灰度图像，记为 $Y_{src}$​。</li><li>将变形后的参考图像 $I~ref$转换为灰度图像，记为 $\tilde{Y}_{ref}$​。</li><li>输入遮挡掩膜 $M_{occ}$​。</li></ul></li><li><strong>融合网络：</strong><ul><li>我们构建了一个 5 层的 UNet 网络，以 $Y_{src}$​、$\tilde{Y}<em>{ref}$​ 和$M</em>{occ}$​ 作为输入，生成融合后的灰度图像 $Y_{fusion}$​。</li><li>该 UNet 的详细架构在附录中提供。</li></ul></li><li><strong>颜色恢复：</strong><ul><li>将融合得到的灰度图像 $Y_{fusion}$​ 与 $I_{src}$​ 的 UV 色彩通道相结合。</li><li>转换回 RGB 空间，生成融合输出图像 $I_{fusion}$​</li></ul></li></ol><h4 class="relative group"><strong>自适应混合（Adaptive Blending）</strong>：<div id=%E8%87%AA%E9%80%82%E5%BA%94%E6%B7%B7%E5%90%88adaptive-blending class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E8%87%AA%E9%80%82%E5%BA%94%E6%B7%B7%E5%90%88adaptive-blending aria-label=Anchor>#</a></span></h4><p>尽管机器学习模型在图像对齐和融合方面具有强大能力，但 W 和 T 之间的不匹配仍可能在输出中引入明显伪影。这些不匹配包括：</p><ol><li><strong>景深差异（DoF differences）</strong></li><li><strong>像素遮挡（Occluded pixels）</strong></li><li><strong>对齐阶段的变形伪影（Warping artifacts）</strong></li></ol><p>为了解决这些问题，我们提出了一种自适应融合策略，通过结合从 <strong>失焦图</strong>、<strong>遮挡图</strong>、<strong>光流不确定性图</strong> 和 <strong>对齐拒绝图</strong> 派生的 <strong>alpha mask</strong>，自适应地将 $Y_{src}$​ 和 $Y_{fusion}$​ 融合。最终输出图像消除了显著伪影，并在 W 和 T 之间像素级一致性存在问题的情况下具有较高鲁棒性。</p><h5 class="relative group">长焦（T）的窄景深问题<div id=%E9%95%BF%E7%84%A6t%E7%9A%84%E7%AA%84%E6%99%AF%E6%B7%B1%E9%97%AE%E9%A2%98 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E9%95%BF%E7%84%A6t%E7%9A%84%E7%AA%84%E6%99%AF%E6%B7%B1%E9%97%AE%E9%A2%98 aria-label=Anchor>#</a></span></h5><p>我们观察到，移动设备的长焦摄像头（T）通常比广角摄像头（W）具有更窄的景深（DoF）。这是因为景深与光圈数和焦距的平方成正比.典型情况下，T 和 W 的焦距比大于 3 倍，而光圈数比小于 2.5 倍。因此，T 的景深通常显著窄于 W. 因此, 需要通过失焦图（defocus map）排除失焦像素区域，以避免伪影的引入。单幅图像的失焦图估计是一个病态问题，通常需要复杂且计算量大的机器学习模型. 为此，我们提出了一种高效算法，重新利用在对齐阶段计算的光流信息生成失焦图：</p><ul><li>光流信息中包含了图像的深度和运动特征，通过分析光流与场景深度的关系，可以高效生成失焦图，而无需额外的复杂模型。</li></ul><p><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/8n90i6w.png alt=xwEY9I7mbXrkEog></figure></p><h5 class="relative group">失焦图（Defocus Map）生成方法<div id=%E5%A4%B1%E7%84%A6%E5%9B%BEdefocus-map%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E5%A4%B1%E7%84%A6%E5%9B%BEdefocus-map%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95 aria-label=Anchor>#</a></span></h5><p>为了估计失焦图，我们需要确定两个关键信息：</p><ol><li><strong>相机的焦点位置</strong>（focused center）：即画面中处于清晰聚焦状态的中心区域。</li><li><strong>每个像素相对于焦点位置的相对深度</strong>（relative depth）。
由于 W 和 T 的视场角（FOV）大致是平行的（fronto-parallel），并且光流的幅值与相机视差成正比，从而与场景深度相关，我们设计了一种基于光流的高效失焦图估计算法，具体过程如下（见图 5）：</li></ol><ul><li><strong>获取焦点感兴趣区域$ROI$：</strong><ul><li>从相机的自动对焦（auto-focus）模块获取焦点区域。该模块提供了一个矩形区域，表示 T 图像中大部分像素聚焦清晰的区域（即焦点 ROI）。</li></ul></li><li><strong>基于光流估计焦点位置 $x_f$​：</strong><ul><li>利用双摄像头立体视觉，将光流视为场景深度的代理。假设在静态场景中，位于同一焦平面的像素具有相似的光流向量（Szeliski 2022）。</li><li>在焦点 ROI 中对光流向量应用 <strong>k-均值聚类算法</strong>，确定聚类密度最高的区域（即最大聚类）。</li><li>聚类中心被定义为焦点位置 $x_f$。</li></ul></li><li><strong>估计像素相对深度并生成失焦图：</strong><ul><li>计算每个像素的光流向量与焦点位置 $x_f$ 的光流向量之间的欧氏距离$L2$ 距离）。</li></ul></li></ul><h5 class="relative group">失焦图计算公式<div id=%E5%A4%B1%E7%84%A6%E5%9B%BE%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E5%A4%B1%E7%84%A6%E5%9B%BE%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F aria-label=Anchor>#</a></span></h5><p>失焦图 $M_{defocus}(x)$的计算公式如下：
$$
M_{defocus}(x) = \text{sigmoid} \left( \frac{| F_{fwd}(x) - F_{fwd}(x_f) |_2^2 - \gamma}{\sigma_f} \right)
$$
其中：</p><ul><li>$F_{fwd}(x)$：像素 ( x ) 的正向光流向量。</li><li>$F_{fwd}(x_f)$ ：焦点位置 ( x_f ) 的正向光流向量。</li><li>$\gamma$：控制容忍范围的阈值。</li><li>$\sigma_f$ ：控制失焦图平滑程度的参数。</li></ul><h5 class="relative group">遮挡图计算公式<div id=%E9%81%AE%E6%8C%A1%E5%9B%BE%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E9%81%AE%E6%8C%A1%E5%9B%BE%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F aria-label=Anchor>#</a></span></h5><p>遮挡图 $M_{occ}(x)$ 的计算基于前向-后向光流一致性（Forward-Backward Flow Consistency），公式如下：
$M_{occ}(x) = \min\left(s \cdot | W(W(x; F_{fwd}); F_{bwd}) - x |_2, 1 \right)$
其中：</p><ul><li>$W$：双线性变形操作符（bilinear warping operator），用来根据光流将图像坐标 $x$ 映射到新的位置。</li><li>$F_{fwd}$：从源图像 $I_{src}$​ 到参考图像 $I_{ref}$​的前向光流。</li><li>$F_{bwd}$​：从参考图像 $I_{ref}$ 到源图像 $I_{src}$​ 的后向光流。</li><li>$s$：一个缩放因子，用于调整光流差异的敏感性。</li><li>$x$：源图像上的 2D 图像坐标。</li></ul><h5 class="relative group">光流不确定性图<div id=%E5%85%89%E6%B5%81%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%9B%BE class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E5%85%89%E6%B5%81%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%9B%BE aria-label=Anchor>#</a></span></h5><p>由于稠密对应（dense correspondence）问题本质上是病态的，我们增强了 PWC-Net 的功能，使其输出光流不确定性图（flow uncertainty map），用于描述每个像素光流预测的不确定性。方法如下：</p><ol><li><p><strong>不确定性建模</strong>：<br>增强后的 PWC-Net 为每个像素预测一个多元拉普拉斯分布（multivariate Laplacian distribution）的光流向量，而非单一的点估计。它预测了两条附加通道，分别为光流在 x 和 y 方向上的对数方差（log-variance），记为 ${Var}_x$​ 和 $\text{Var}_y$​。</p></li><li><p><strong>转换为像素单位</strong>：<br>对数方差通过以下公式转换为像素单- 这里，S(x)S(x)S(x) 是每个像素的光流不确定性值。
$$
S(x) = \sqrt{\exp(\log(\text{Var}_x(x))) + \exp(\log(\text{Var}_y(x)))}
$$</p></li><li><p><strong>生成光流不确定性图</strong>：<br>$$
M_{\text{flow}}(x) = \frac{\min(S(x), s_{\text{max}})}{s_{\text{max}}}
$$
光流不确定性图通常在物体边界或无纹理区域（texture-less regions）中具有较高值。</p></li></ol><h5 class="relative group">对齐拒绝图（Alignment Rejection Map）<div id=%E5%AF%B9%E9%BD%90%E6%8B%92%E7%BB%9D%E5%9B%BEalignment-rejection-map class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E5%AF%B9%E9%BD%90%E6%8B%92%E7%BB%9D%E5%9B%BEalignment-rejection-map aria-label=Anchor>#</a></span></h5><p>为了排除由于<strong>对齐错误引入的伪影</strong>，我们通过对比源图像和参考图像（对齐后）的局部相似性生成对齐拒绝图（alignment rejection map）。方法如下：</p><ul><li><strong>匹配光学分辨率</strong>：<ul><li>使用双线性插值调整对齐后的参考帧 $\tilde{Y}<em>{\text{ref}}$​ 的分辨率，使其与 W 的光学分辨率匹配，得到下采样图像 $\tilde{Y}</em>{\text{ref}}^\downarrow$​。</li></ul></li><li><strong>计算局部差异</strong>：<ul><li>对于源图像局部补丁 $P_{\text{src}}$​ 和对齐参考图像补丁 $P_{\tilde{\text{ref}}}$​：<ol><li>减去补丁的均值$u_{\text{src}}$​ 和 $u_{\text{ref}}$。</li><li>计算归一化差异：
$$
P_\delta = (P_{\text{src}} - \mu_{\text{src}}) - (P_{\tilde{\text{ref}}} - \mu_{\text{ref}})
$$</li></ol></li><li>生成对齐拒绝图
$$
M_{\text{reject}}(x) = 1 - \exp\left(-\frac{|P_\delta(x)|<em>2^2}{\sigma</em>{\text{src}}^2(x) + \epsilon_0}\right)
$$</li></ul></li></ul><h4 class="relative group">最终混合<div id=%E6%9C%80%E7%BB%88%E6%B7%B7%E5%90%88 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%E6%9C%80%E7%BB%88%E6%B7%B7%E5%90%88 aria-label=Anchor>#</a></span></h4><p>$$
M_{\text{blend}} = \max(1 - M_{\text{occ}} - M_{\text{defocus}} - M_{\text{flow}} - M_{\text{reject}}, 0)
$$
最终输出图像通过 Alpha 混合生成，并裁剪回完整的 W 图像：
$$
I_{\text{final}} = \text{uncrop}\left(M_{\text{blend}} \odot I_{\text{fusion}} + (1 - M_{\text{blend}}) \odot I_{\text{src}}, W\right)
$$
通过结合多种遮罩（如遮挡图、失焦图、光流不确定性图和对齐拒绝图），我们确保了融合结果的高质量和鲁棒性，避免了伪影、模糊或对齐错误的引入。</p><h3 class="relative group">Implementation Details<div id=implementation-details class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation-details aria-label=Anchor>#</a></span></h3><h2 class="relative group">Results and Evaluation<div id=results-and-evaluation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#results-and-evaluation aria-label=Anchor>#</a></span></h2><h3 class="relative group">Experiments:<div id=experiments class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#experiments aria-label=Anchor>#</a></span></h3><h3 class="relative group">Results<div id=results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#results aria-label=Anchor>#</a></span></h3><h3 class="relative group">Comparisons<div id=comparisons class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#comparisons aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/rlLj4d0.png alt=WM6ILjWeYsM35Sc></figure></p><p><figure><img class="my-0 rounded-md" loading=lazy src=https://i.imgur.com/NjA4JmF.png alt=X8MjkLL7M9laHpI></figure></p><h2 class="relative group">Critical Analysis<div id=critical-analysis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#critical-analysis aria-label=Anchor>#</a></span></h2><h3 class="relative group">Strengths<div id=strengths class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#strengths aria-label=Anchor>#</a></span></h3><h3 class="relative group">Weaknesses<div id=weaknesses class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#weaknesses aria-label=Anchor>#</a></span></h3><h3 class="relative group">Insights<div id=insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#insights aria-label=Anchor>#</a></span></h3><h2 class="relative group">Broader Impact and Future Work<div id=broader-impact-and-future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#broader-impact-and-future-work aria-label=Anchor>#</a></span></h2><h3 class="relative group">Broader Implications<div id=broader-implications class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#broader-implications aria-label=Anchor>#</a></span></h3><h3 class="relative group">Open Questions<div id=open-questions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#open-questions aria-label=Anchor>#</a></span></h3><h3 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h3><h2 class="relative group">Personal Reflection<div id=personal-reflection class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#personal-reflection aria-label=Anchor>#</a></span></h2><h3 class="relative group">Relevance to My Work<div id=relevance-to-my-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#relevance-to-my-work aria-label=Anchor>#</a></span></h3><h3 class="relative group">Practical Applications<div id=practical-applications class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#practical-applications aria-label=Anchor>#</a></span></h3><h3 class="relative group">Key Learnings<div id=key-learnings class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-learnings aria-label=Anchor>#</a></span></h3><h2 class="relative group">Related Work and Context<div id=related-work-and-context class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#related-work-and-context aria-label=Anchor>#</a></span></h2><h3 class="relative group">Prior Work<div id=prior-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#prior-work aria-label=Anchor>#</a></span></h3><p><strong>基于学习的单幅图像超分辨率（SISR）</strong><br>过去十年中，多种方法（例如 Christian Ledig 2017；Dong et al. 2014；Kim et al. 2016；Lai et al. 2017；Wang et al. 2018；Xu et al. 2023；Zhang et al. 2019a, 2022b, 2018）在单幅图像超分辨率领域展示了出色的效果。然而，由于该任务的严重病态特性，这些方法在<strong>较大的上采样因子（例如智能手机混合变焦常见的 2-5 倍）下会生成模糊的细节</strong>。此外，一些方法仅适用于特定领域，如人脸超分辨率（Chan et al. 2021；Gu et al. 2020；He et al. 2022；Menon et al. 2020）。</p><p><strong>基于互联网图像的参考超分辨率（RefSR）</strong><br>RefSR 通过低分辨率输入和一个或多个高分辨率参考图像（Pesavento et al. 2021）生成高分辨率结果。传统的 RefSR 方法假设参考图像来自互联网（Sun and Hays 2012）或在同一事件的不同时间、位置或相机型号拍摄（Wang et al. 2016；Zhang et al. 2019b），研究的重点是改进源图像与参考图像之间的密集对齐（Huang et al. 2022；Jiang et al. 2021；Xia et al. 2022；Zheng et al. 2018）或增强对不相关参考图像的鲁棒性（Lu et al. 2021；Shim et al. 2020；Xie et al. 2020；Yang et al. 2020；Zhang et al. 2019b）。<br>与此不同，我们通过同步捕获 W 和 T 图像来减轻对齐的挑战，避免了由于物体运动引起的对齐问题。</p><p><strong>基于辅助摄像头的参考超分辨率（RefSR）</strong><br>最近的一些 RefSR 研究（Trinidad et al. 2019；Wang et al. 2021；Zhang et al. 2022a）通过辅助摄像头捕获同一场景的参考图像。然而，由于缺乏像素对齐的输入和真实标签图像对，PixelFusionNet（Trinidad et al. 2019）通过退化模型从高分辨率参考图像合成低分辨率输入，并使用像素级损失（如 l1 和 VGG 损失）进行训练。然而，这种模型在面对<strong>真实场景输入时表现不佳</strong>，主要原因是训练和推理阶段图像之间的域差异。</p><p>另一方面，SelfDZSR（Zhang et al. 2022a）、DCSR（Wang et al. 2021）和 RefVSR（Lee et al. 2022）将参考图像视为训练或微调的目标。我们观察到，这种训练设置容易陷入退化的局部最小值：模型通常会学习<strong>恒等映射</strong>（identity mapping），仅将 T 图像的内容复制到输出中。这会导致严重的对齐错误、颜色偏移和景深不匹配等问题，这在实际摄影中是无法接受的。</p><p>为了解决这些问题，我们在训练过程中额外捕获一张 T 图像，从而缓解上述问题并提高训练的鲁棒性。</p><p><strong>高效的移动设备参考超分辨率（RefSR）</strong><br>现有方法通常因使用注意力机制/Transformer（如 Wang et al. 2021；Yang et al. 2020）或深度网络架构（如 Zhang et al. 2022a）而占用大量内存。这些方法在处理 12MP 输入分辨率时，即使在拥有 40GB RAM 的 NVIDIA A100 桌面 GPU 上也可能遇到内存不足（OOM）问题，更<strong>无法在移动设备上运行</strong>。相比之下，我们的系统在移动 GPU 上处理 12MP 输入仅需 500 毫秒，使用内存仅为 300MB。</p><p>我们的系统设计受参考图像人脸去模糊方法 [Lai et al. 2022] 的启发，但我们解决的问题更具挑战性：</p><ol><li><strong>通用场景的超分辨率</strong>：我们将超分辨率应用于通用图像，而非仅专注于人脸。因此，我们的系统需要对多样化的场景更具鲁棒性，并能够处理来自两个摄像头的各种不完美和不匹配问题。</li><li><strong>训练数据的域差异</strong>：与可以从合成数据中学习的人脸去模糊模型不同，图像超分辨率模型对训练数据的域差异更为敏感。此外，为基于参考的超分辨率任务收集真实训练数据也更加困难。因此，我们提出的自适应混合方法和双手机平台设计，成为了我们与 [Lai et al. 2022] 方法的关键区别</li></ol><h3 class="relative group">Historical Context<div id=historical-context class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#historical-context aria-label=Anchor>#</a></span></h3><h2 class="relative group">References<div id=references class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#references aria-label=Anchor>#</a></span></h2></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/&amp;title=Efficient%20Hybrid%20Zoom%20using%20Camera%20Fusion%20on%20Mobile%20Phones%20Reading%20Notes" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/&amp;text=Efficient%20Hybrid%20Zoom%20using%20Camera%20Fusion%20on%20Mobile%20Phones%20Reading%20Notes" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://api.whatsapp.com/send?text=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/&amp;resubmit=true&amp;title=Efficient%20Hybrid%20Zoom%20using%20Camera%20Fusion%20on%20Mobile%20Phones%20Reading%20Notes" title="Share via WhatsApp" aria-label="Share via WhatsApp"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4.0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3.0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2.0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2.0-101.7 82.8-184.5 184.6-184.5 49.3.0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5.0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8s-14.3 18-17.6 21.8c-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2s-9.7 1.4-14.8 6.9c-5.1 5.6-19.4 19-19.4 46.3.0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://pinterest.com/pin/create/bookmarklet/?url=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/&amp;description=Efficient%20Hybrid%20Zoom%20using%20Camera%20Fusion%20on%20Mobile%20Phones%20Reading%20Notes" title="Pin on Pinterest" aria-label="Pin on Pinterest"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/&amp;resubmit=true&amp;title=Efficient%20Hybrid%20Zoom%20using%20Camera%20Fusion%20on%20Mobile%20Phones%20Reading%20Notes" title="Submit to Reddit" aria-label="Submit to Reddit"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer/sharer.php?u=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/&amp;quote=Efficient%20Hybrid%20Zoom%20using%20Camera%20Fusion%20on%20Mobile%20Phones%20Reading%20Notes" title="Share on Facebook" aria-label="Share on Facebook"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://teiboku.github.io/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/&amp;subject=Efficient%20Hybrid%20Zoom%20using%20Camera%20Fusion%20on%20Mobile%20Phones%20Reading%20Notes" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_posts/Efficient Hybird Zoom using Camera Fusion on Mobile Phones.md",oid_likes="likes_posts/Efficient Hybird Zoom using Camera Fusion on Mobile Phones.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/test/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"></span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time>
</span></span></a></span><span></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://teiboku.github.io/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>