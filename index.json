[{"content":" ğŸŒ§ï¸ CLIPRPN: ç”ŸæˆAIæ™‚ä»£ã®æ–°ã—ã„ã€Œé›¨ã€ç†è§£ãƒ¢ãƒ‡ãƒ« # â€• ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå­¦ç¿’ã«ã‚ˆã‚‹é›¨ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã¨é™¤é›¨ â€•\nè¿‘å¹´ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚„CLIPã«ä»£è¡¨ã•ã‚Œã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã®é€²åŒ–ã«ã‚ˆã‚Šã€è¦–è¦šã¨è¨€èªã®çµ±åˆç†è§£ãŒæ€¥é€Ÿã«ç™ºå±•ã—ã¦ã„ã¾ã™ã€‚æœ¬ç ”ç©¶ã€ŒCLIPRPNï¼ˆCLIP-based Rain Pattern Networkï¼‰ã€ã¯ã€ã“ã®æ½®æµã‚’ç”»åƒå¾©å…ƒåˆ†é‡ã€ç‰¹ã«ç”»åƒé™¤é›¨ï¼ˆImage Derainingï¼‰ã«å¿œç”¨ã—ãŸæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚\nğŸ§© èƒŒæ™¯ï¼šå˜ç´”ãªé™¤é›¨ã‹ã‚‰ã€Œç†è§£ã™ã‚‹é™¤é›¨ã€ã¸ # å¾“æ¥ã®é™¤é›¨ãƒ¢ãƒ‡ãƒ«ã¯ã€CNNã‚„Transformerã‚’ç”¨ã„ã¦ä½ãƒ¬ãƒ™ãƒ«ãªç‰¹å¾´ï¼ˆè¼åº¦ãƒ»ã‚¨ãƒƒã‚¸ãƒ»å‹¾é…ï¼‰ã‚’æŠ½å‡ºã—ã€é›¨ streak ã®é™¤å»ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã—ãŸã€‚ã—ã‹ã—ã€ã“ã†ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€Œé›¨ã€ã®å¤šæ§˜ãªå½¢æ…‹ â€• éœ§çŠ¶ã®é›¨ã€æ–œã‚ã® streakã€æ°´æ»´ã«ã‚ˆã‚‹æ•£ä¹± â€• ã‚’ç†è§£ã§ããšã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ã‚„æ§‹é€ ã®éè£œæ­£ã‚’æ‹›ãå•é¡ŒãŒã‚ã‚Šã¾ã—ãŸã€‚\nCLIPRPN ã¯ã“ã®å•é¡Œã«å¯¾ã—ã€ã€Œé›¨ã®ç‰©ç†çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’â€œèªè­˜â€ã—ã¦ã‹ã‚‰é™¤å»ã™ã‚‹ã€ã¨ã„ã†æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã—ã¾ã™ã€‚\nğŸ§  ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ï¼šCLIPã«ã‚ˆã‚‹é›¨ãƒ‘ã‚¿ãƒ¼ãƒ³æ„ŸçŸ¥ # æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€OpenAI ã® CLIPï¼ˆContrastive Languageâ€“Image Pretrainingï¼‰ ã‚’åŸºç›¤ã¨ã—ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«è¡¨ç¾ã‚’åˆ©ç”¨ã—ã¦ã€å…¥åŠ›ç”»åƒä¸­ã®é›¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¨å®šã—ã¾ã™ã€‚ â€¢\tRain Pattern Encoderï¼ˆRPEï¼‰ï¼šCLIPã®è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡ºã—ã€ã€Œé›¨ã€ã‚‰ã—ã•ã‚’å®šé‡åŒ–ã€‚ â€¢\tPrompt Generatorï¼ˆPGï¼‰ï¼šãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ã€Œéœ§çŠ¶ã®é›¨ã€ã€Œæ–œã‚ã«é™ã‚‹é›¨ã€ã€Œãƒ¬ãƒ³ã‚ºã«ä»˜ç€ã—ãŸæ°´æ»´ã€ãªã©ã®æ½œåœ¨çš„è¡¨ç¾ã‚’ç”Ÿæˆã€‚ â€¢\tDeraining Networkï¼ˆDNï¼‰ï¼šæ¨å®šã•ã‚ŒãŸé›¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¡ä»¶æƒ…å ±ã¨ã—ã¦æ´»ç”¨ã—ã€æ§‹é€ ä¿æŒå‹ã®é™¤é›¨ã‚’å®Ÿç¾ã€‚\nã“ã®ã‚ˆã†ã«ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã¯â€œç”»åƒã«å«ã¾ã‚Œã‚‹é›¨ã®æ„å‘³â€ã‚’ç†è§£ã—ã€å˜ãªã‚‹ãƒã‚¤ã‚ºé™¤å»ã§ã¯ãªãã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ãŸé™¤é›¨ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\nğŸ”¬ å®Ÿé¨“ã¨çµæœ # è¤‡æ•°ã®å®Ÿé¨“ï¼ˆRain100Hã€Rain200Lã€SPA-Dataãªã©ï¼‰ã«ãŠã„ã¦ã€CLIPRPN ã¯å¾“æ¥ã®SOTAãƒ¢ãƒ‡ãƒ«ï¼ˆMPRNet, DGUNet, Restormerç­‰ï¼‰ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚ â€¢\tPSNR/SSIM ãŒå¹³å‡ +0.6ã€œ1.2 å‘ä¸Š â€¢\tè¦–è¦šçš„ã«è‡ªç„¶ãªãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ä¿æŒ â€¢\tç•°ãªã‚‹é›¨ã‚¿ã‚¤ãƒ—ã«å¯¾ã™ã‚‹ä¸€èˆ¬åŒ–èƒ½åŠ›\nã•ã‚‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè»¢ç§»ã«ã‚‚å¼·ãã€æœªå­¦ç¿’ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚å®‰å®šã—ãŸå¾©å…ƒå“è³ªã‚’ç¶­æŒã—ã¾ã—ãŸã€‚\nğŸ§­ æ„ç¾©ã¨å±•æœ›\nCLIPRPN ã®æœ€å¤§ã®æ„ç¾©ã¯ã€ã€Œé«˜ãƒ¬ãƒ™ãƒ«ãªæ¦‚å¿µç†è§£ã‚’ä½ãƒ¬ãƒ™ãƒ«å¾©å…ƒã‚¿ã‚¹ã‚¯ã«å°å…¥ã€ã—ãŸç‚¹ã«ã‚ã‚Šã¾ã™ã€‚ ä»Šå¾Œã¯ã€é›¨ã ã‘ã§ãªãã€é›ªãƒ»éœ§ãƒ»å¤œé–“åå°„ãªã©ä»–ã®æ°—è±¡åŠ£åŒ–ã«ã‚‚å¿œç”¨å¯èƒ½ã§ã‚ã‚Šã€ãƒ“ãƒ‡ã‚ªé ˜åŸŸã¸ã®æ‹¡å¼µï¼ˆæ™‚ç©ºé–“CLIPèª˜å°ï¼‰ã‚‚æœŸå¾…ã•ã‚Œã¾ã™ã€‚\n","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/projects/1760784774221-clip%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E9%9B%A8%E6%B0%B4%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3%E8%AA%8D%E8%AD%98%E3%81%AB%E3%82%88%E3%82%8B%E7%94%BB%E5%83%8F%E9%99%A4%E9%9B%A8%E3%83%A2%E3%83%87%E3%83%AB/","section":"Projects","summary":"A derain model use CLIP to auto routing.","title":"CLIPã«åŸºã¥ãé›¨æ°´ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã«ã‚ˆã‚‹ç”»åƒé™¤é›¨ãƒ¢ãƒ‡ãƒ«","type":"projects"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":" ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®è¤‡çœ¼æ™‚ä»£ã«ãŠã‘ã‚‹æ–°ã—ã„ç”»åƒå¾©å…ƒã‚¢ãƒ—ãƒ­ãƒ¼ãƒ ğŸ“± æ¦‚è¦ # è¿‘å¹´ã€ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚«ãƒ¡ãƒ©ãªã©ã®ãƒãƒ«ãƒã‚«ãƒ¡ãƒ©ãƒ‡ãƒã‚¤ã‚¹ã¯ã€åºƒè§’ã¨æœ›é ã¨ã„ã£ãŸç•°ãªã‚‹ç„¦ç‚¹è·é›¢ã®ç”»åƒã‚’åŒæ™‚ã«å–å¾—ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚\nã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ç”»åƒã«ã¯è¦–é‡ï¼ˆFoVï¼‰ã®ä¸ä¸€è‡´ã‚„è§£åƒåº¦ãƒ»è‰²ã®ãƒ‰ãƒ¡ã‚¤ãƒ³å·®ãŒå­˜åœ¨ã—ã€å˜ç´”ãªç”»åƒèåˆã§ã¯é«˜ç²¾ç´°ãªå†æ§‹æˆãŒå›°é›£ã§ã—ãŸã€‚\næœ¬ç ”ç©¶ã§ææ¡ˆã•ã‚ŒãŸ DMÂ³Netï¼ˆDual-camera SR via Domain Modulation and Multi-scale Matchingï¼‰ã¯ã€ã“ã®èª²é¡Œã«å¯¾ã—ã¦ã€Œå¤šæ®µéšã®ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°ã€ã¨ã€Œã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ‰ãƒ¡ã‚¤ãƒ³æ•´åˆã€ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€å¾“æ¥ã‚’å‡Œé§•ã™ã‚‹è¶…è§£åƒæ€§èƒ½ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚ ğŸ” èƒŒæ™¯ã¨èª²é¡Œ # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚«ãƒ¡ãƒ©ã§ã¯ã€ â€¢\tåºƒè§’ã‚«ãƒ¡ãƒ©ï¼šåºƒã„è¦–é‡ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãŒä½è§£åƒåº¦ â€¢\tæœ›é ã‚«ãƒ¡ãƒ©ï¼šç‹­ã„è¦–é‡ãªãŒã‚‰é«˜è§£åƒåº¦ã®ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ã‚’å«ã‚€\nã“ã®æœ›é ç”»åƒã‚’ã€Œå‚ç…§ç”»åƒï¼ˆReferenceï¼‰ã€ã¨ã—ã¦åˆ©ç”¨ã—ã€åºƒè§’ç”»åƒã‚’é«˜ç²¾ç´°åŒ–ã™ã‚‹ã®ãŒ**ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚«ãƒ¡ãƒ©è¶…è§£åƒï¼ˆDual-Camera SRï¼‰**ã®ç›®çš„ã§ã™ã€‚ å¾“æ¥ã®æ‰‹æ³•ï¼ˆä¾‹ï¼šDCSR, KeDuSRï¼‰ã¯ãƒ‘ãƒƒãƒå˜ä½ã®ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚Šé«˜å‘¨æ³¢æƒ…å ±ã‚’è»¢é€ã—ã¾ã™ãŒã€ã‚¹ã‚±ãƒ¼ãƒ«ã®åˆ¶ç´„ã«ã‚ˆã‚Šç´°éƒ¨å†ç¾æ€§ãŒä½ä¸‹ã™ã‚‹å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ ï¿¼ã€‚\nğŸ§© ææ¡ˆæ‰‹æ³•ï¼šDMÂ³Net ã®ä¸»è¦æ§‹æˆ # DMÂ³Net ã¯ä»¥ä¸‹ã®3ã¤ã®è¦ç´ ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n1ï¸âƒ£ Multi-Scale Matchingï¼ˆãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°ï¼‰\nè¤‡æ•°ã®å—å®¹é‡ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ1/4, 1/2, 1ï¼‰ã§ç‰¹å¾´ãƒ‘ãƒƒãƒã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€æœ€é©ãªå‚ç…§ãƒ‘ãƒƒãƒã‚’é¸æŠã€‚ ã“ã‚Œã«ã‚ˆã‚Šã€å¾“æ¥ã®ã‚·ãƒ³ã‚°ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°ã‚ˆã‚Šã‚‚æ§‹é€ æ•´åˆæ€§ã¨ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«å†ç¾æ€§ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã—ãŸã€‚\nã•ã‚‰ã«ã€ã€ŒKey Pruningã€æˆ¦ç•¥ã‚’å°å…¥ã—ã€é¡ä¼¼åº¦ã®é«˜ã„å†—é•·ãƒ‘ãƒƒãƒã‚’å‰Šé™¤ã€‚ GPUãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’ç´„17%å‰Šæ¸›ã—ã¤ã¤ã€æ¨è«–æ™‚é–“ã‚’3åˆ†ã®1ã«çŸ­ç¸® ï¿¼ã€‚\n2ï¸âƒ£ Domain Modulationï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³å¤‰èª¿ï¼‰\nåºƒè§’ç”»åƒï¼ˆLRï¼‰ã¨æœ›é ç”»åƒï¼ˆRefï¼‰ã¯ã€ç•°ãªã‚‹æ’®å½±ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚Šãƒ‰ãƒ¡ã‚¤ãƒ³å·®ãŒç”Ÿã˜ã¾ã™ã€‚ ãã“ã§ã€DMÂ³Net ã¯ãã‚Œãã‚Œã®ãƒšã‚¢ï¼ˆLR-GT, LRc-Refï¼‰ã‹ã‚‰**ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³åŸ‹ã‚è¾¼ã¿ï¼ˆz, zgtï¼‰**ã‚’å­¦ç¿’ã€‚ ä¸¡è€…ã®åˆ†å¸ƒã‚’æ•´åˆã•ã›ã‚‹ã€Œãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¢ã‚¦ã‚§ã‚¢æå¤±ï¼ˆL_domainï¼‰ã€ã‚’å°å…¥ã—ã€å†æ§‹æˆéç¨‹ã«é©å¿œçš„ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€é«˜è§£åƒãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®æ»‘ã‚‰ã‹ãªå†™åƒã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚\n3ï¸âƒ£ Integrated Reconstructionï¼ˆçµ±åˆå†æ§‹æˆï¼‰\næœ€çµ‚çš„ã«ã€ãƒãƒƒãƒãƒ³ã‚°ã§å†æ§‹æˆã—ãŸé«˜å‘¨æ³¢ç‰¹å¾´ã¨ã€ãƒ‰ãƒ¡ã‚¤ãƒ³å¤‰èª¿ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿æƒ…å ±ã‚’çµ±åˆã—ã€è¦–è¦šçš„ã«è‡ªç„¶ã§é«˜å¿ å®Ÿåº¦ãªHRç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚\nğŸ§ª å®Ÿé¨“çµæœ # 3ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆDuSR-Real, RealMCVSR-Real, CameraFusion-Realï¼‰ã«ãŠã„ã¦ã€DMÂ³Netã¯ä»¥ä¸‹ã®æ€§èƒ½ã‚’é”æˆï¼š ç‰¹ã«éé‡è¤‡é ˜åŸŸï¼ˆCorner Regionï¼‰ã«ãŠã„ã¦ã‚‚é«˜ç²¾åº¦ã‚’ç¶­æŒã—ã€éƒ¨åˆ†çš„ãªè¦–é‡å·®ãƒ»è‰²å·®ã«å¯¾ã—ã¦ã‚‚é ‘å¥ã«å‹•ä½œã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ ï¿¼ã€‚\nâš™ï¸ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨è€ƒå¯Ÿ # â€¢\tãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«åŒ–ï¼šã‚¹ã‚±ãƒ¼ãƒ«ã‚’å¢—ã‚„ã™ã»ã©PSNRãŒå‘ä¸Šï¼ˆ27.75 dBï¼‰ã€‚ â€¢\tãƒ‰ãƒ¡ã‚¤ãƒ³å¤‰èª¿ï¼šz ã¨ zgt ã‚’ä½µç”¨ã—ãŸå ´åˆãŒæœ€è‰¯æ€§èƒ½ã€‚ â€¢\tKey Pruningï¼šé–¾å€¤0.7ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”16ã§æœ€é©ãƒãƒ©ãƒ³ã‚¹ã€‚ ã“ã‚Œã‚‰ã®çµæœã‹ã‚‰ã€DMÂ³Net ã®æ€§èƒ½å‘ä¸Šã¯å±€æ‰€çš„ãƒãƒƒãƒãƒ³ã‚°ã®ç²¾åº¦ã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³æ•´åˆã®å°å…¥ã®ä¸¡ç«‹ã«ã‚ˆã‚‹ã‚‚ã®ã§ã‚ã‚‹ã¨åˆ†æã•ã‚Œã¦ã„ã¾ã™ã€‚\nğŸŒ å¿œç”¨ã¨å±•æœ› # DMÂ³Net ã¯ã€ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ»ãƒ‰ãƒ­ãƒ¼ãƒ³ãƒ»è»Šè¼‰ã‚«ãƒ¡ãƒ©ãªã©è¤‡æ•°ãƒ¬ãƒ³ã‚ºã‚’æŒã¤æ’®å½±ç’°å¢ƒã«ç›´æ¥å¿œç”¨å¯èƒ½ã§ã™ã€‚ ç‰¹ã«ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢é–“ã§ç•°ãªã‚‹ç”»è§’ãƒ»è§£åƒåº¦ãƒ»ISPå‡¦ç†ã‚’çµ±åˆã™ã‚‹**è¨ˆç®—å†™çœŸæŠ€è¡“ï¼ˆComputational Photographyï¼‰**ã«ãŠã„ã¦é«˜ã„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\nä»Šå¾Œã¯ã€ãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åŒ–ã‚’ç›®æŒ‡ã—ã€Diffusionå‹å¾©å…ƒã‚„å‹•ç”»æ‹¡å¼µã¸ã®é©ç”¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚\n","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/projects/1760786004876-%E3%83%87%E3%83%A5%E3%82%A2%E3%83%AB%E3%82%AB%E3%83%A1%E3%83%A9%E7%94%BB%E5%83%8F%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B%E8%B6%85%E8%A7%A3%E5%83%8F%E6%89%8B%E6%B3%95/","section":"Projects","summary":"ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®è¤‡çœ¼ã‚«ãƒ¡ãƒ©ã‚’æ´»ç”¨ã—ãŸé«˜ç²¾ç´°ç”»åƒç”ŸæˆæŠ€è¡“ DMÂ³Net ã®ç´¹ä»‹","title":"DMÂ³Net: ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚«ãƒ¡ãƒ©è¶…è§£åƒã®æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ","type":"projects"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/tags/dual-camera/","section":"Tags","summary":"","title":"Dual Camera","type":"tags"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/tags/example/","section":"Tags","summary":"","title":"Example","type":"tags"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/tags/super-resolution/","section":"Tags","summary":"","title":"Super Resolution","type":"tags"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/tags/tag/","section":"Tags","summary":"","title":"Tag","type":"tags"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2025å¹´10æœˆ18æ—¥","externalUrl":null,"permalink":"/","section":"ã‹ã‚“ãã†","summary":"","title":"ã‹ã‚“ãã†","type":"page"},{"content":" Abstract # DSLR cameras can achieve various zoom levels by changing the lens distance or swapping lens types. However, due to space constraints, these techniques are not feasible on mobile devices. Most smartphones adopt a hybrid zoom system: typically using a wide-angle (W) camera for low zoom levels and a telephoto (T) camera for high zoom levels. To simulate zoom levels between W and T, these systems crop and digitally enlarge images from W, resulting in significant detail loss. In this paper, we propose an efficient hybrid zoom super-resolution system for mobile devices. This system captures synchronized W and T image pairs and utilizes a machine learning model to align and transfer details from T to W. We further develop an adaptive blending method that can handle depth of field mismatches, scene occlusions, motion uncertainties, and alignment errors. To minimize domain differences, we design a dual smartphone camera setup to capture real scene inputs and real label data for supervised training. In extensive evaluations on real scenes, our method can generate 12-megapixel images on mobile platforms in 500 milliseconds, achieving TA.\nKey word # hybrid zoom, dual camera fusion, deep neural networks\nKey Takeaways # Problem Addressed # Almost all upsampling methods (such as bilinear interpolation, bicubic interpolation, etc.) lead to varying degrees of image quality degradation. Consequently, numerous methods have emerged that utilize high zoom T images as references to add real details to low zoom W images. Commercial solutions are not publicly available, and academic research is relatively inefficient. These methods often perform poorly on mobile devices, are susceptible to defects in reference images, and may introduce domain differences between training and inference. To address these issues, we propose a Hybrid Zoom Super-Resolution (HZSR) system.\nMain Contributions # A machine learning-based hybrid zoom super-resolution (HZSR) system that operates efficiently on mobile devices and exhibits strong robustness to imperfections in real scene images (see Section 3). A training strategy that minimizes domain differences through a dual smartphone camera platform and avoids learning trivial mappings in reference super-resolution (RefSR) tasks (see Section 4). A high-quality synchronized dataset containing 150 pairs of high-resolution (12MP) W and T images, named the Hzsr dataset, which will be released on our project website for future research (see Section 5). Methodology # Approach # Adapting to Imperfect Reference Images # We propose an efficient defocus detection algorithm that excludes defocused areas based on the correlation between scene depth and optical flow. By combining defocus maps, alignment errors, optical flow uncertainties, and scene occlusion information, we design an adaptive blending mechanism to generate high-quality, artifact-free super-resolution results.\nMinimizing Domain Differences through Real Scene Inputs # In reference super-resolution (RefSR) tasks, collecting perfectly aligned W/T image pairs as training data is very challenging. Therefore, previous research has explored two possible but suboptimal solutions:\nUsing the reference image T as the training target (e.g., Wang et al. 2021 and Zhang et al. 2022a), which may transmit defects from the reference image or lead the network to learn identity mapping. Synthesizing low-resolution inputs from target images using degradation models (e.g., Trinidad et al. 2019 and Zhang et al. 2022a), but this method introduces domain differences between training and inference stages, thereby reducing super-resolution performance on real scene images. To avoid learning identity mappings and minimize domain differences, we adopt a design where, during training, a second smartphone of the same model mounted on the photography platform synchronously captures additional T images as references (see Figure 6). With this design, the fusion model uses real W images as input during both training and inference, eliminating domain differences. Additionally, the reference and target images are captured by T cameras from different devices to prevent the network from learning identity mappings. Compared to existing dual zoom RefSR datasets, our design has significant advantages:\nSome datasets exhibit strong temporal motion between W and T (e.g., Wang et al. 2021); Some datasets are limited to static scenes (e.g., Wei et al. 2020). We have collected a large-scale dataset containing high-quality W/T synchronized data, covering dynamic scenes, including portraits, architecture, landscapes, as well as challenging scenes with moving objects and nighttime scenarios. Experiments show that our method outperforms current state-of-the-art methods on both existing dual zoom RefSR datasets and our new dataset. Model/Algorithm # When the user adjusts the zoom to a medium range (e.g., 3-5 times) and presses the shutter button, the system captures a pair of synchronized images. The processing flow is as follows:\nImage Alignment: # First, we perform global coarse alignment through keypoint matching, followed by local dense alignment using optical flow (see Section 3.1). Coarse Alignment:\nFirst, we crop the image from the wide-angle camera (W) to match the field of view (FOV) of the telephoto camera (T). Next, we use bicubic interpolation to adjust the spatial resolution of W to match T (4kÃ—3k). Then, we estimate the global 2D translation vector using the FAST feature point matching algorithm (Rosten and Drummond 2006) and adjust the cropped W to obtain the adjusted image \\(I_{src}\\). Dense Alignment:\nWe use PWC-Net to estimate the dense optical flow between \\(I_{src}\\) and \\(I_{ref}\\). It is important to note that the average offset between W and T at 12MP resolution is about 150 pixels, which is much larger than the motion range in most optical flow training data. Therefore, the optical flow estimated from 12MP images is often very noisy. To improve accuracy, we downsample \\(I_{src}\\) and \\(I_{ref}\\) to a smaller resolution of 384Ã—512 to predict the optical flow, then upsample the flow to the original resolution and deform \\(I_{ref}\\) to obtain \\(\\tilde{I}{ref}\\) using bilinear resampling. This method provides more accurate and robust optical flow estimates at a smaller scale. Optimization To accommodate the computational resource limitations of mobile devices, we removed the DenseNet structure from the original PWC-Net, reducing the model size by 50%, latency by 56%, and peak memory by 63%. Although the endpoint error (EPE) of optical flow increased by 8% on the Sintel dataset, the visual quality of the optical flow remains similar.\nAdditionally, we generate an occlusion map \\(M{occ}\\) using forward-backward consistency checks (Alvarez et al. 2007) to identify areas occluded during alignment.\nImage Fusion: # We use UNet (Ronneberger et al. 2015) to fuse the brightness channel of the cropped image from the wide-angle camera (W) with the deformed reference image from the telephoto camera (T) (see Section 3.2).\nUsing the source image, the deformed reference image, and the occlusion mask as inputs Fusion Process:\nTo maintain color consistency in the W image, we perform fusion only in the luminance space. The specific implementation is as follows:\nInput Preparation: Convert the source image \\(I_{src}\\) to a grayscale image, denoted as \\(Y_{src}\\). Convert the deformed reference image \\(I_{ref}\\) to a grayscale image, denoted as \\(\\tilde{Y}_{ref}\\). Input the occlusion mask \\(M_{occ}\\). Fusion Network: We construct a 5-layer UNet network, using \\(Y_{src}\\), \\(\\tilde{Y}{ref}\\), and \\(M{occ}\\) as inputs to generate the fused grayscale image \\(Y_{fusion}\\). The detailed architecture of this UNet is provided in the appendix. Color Recovery: Combine the fused grayscale image \\(Y_{fusion}\\) with the UV color channels of \\(I_{src}\\). Convert back to RGB space to generate the fused output image \\(I_{fusion}\\). Adaptive Blending: # Although machine learning models have strong capabilities in image alignment and fusion, mismatches between W and T can still introduce noticeable artifacts in the output. These mismatches include:\nDepth of Field differences Occluded pixels Warping artifacts during the alignment stage To address these issues, we propose an adaptive blending strategy that combines alpha masks derived from defocus maps, occlusion maps, optical flow uncertainty maps, and alignment rejection maps to adaptively blend \\(Y_{src}\\) and \\(Y_{fusion}\\). The final output image eliminates significant artifacts and maintains high robustness in cases where pixel-level consistency issues exist between W and T.\nThe Narrow Depth of Field Issue of Telephoto (T) # We observe that the telephoto camera (T) on mobile devices typically has a narrower depth of field (DoF) than the wide-angle camera (W). This is because depth of field is proportional to the square of the f-stop number and focal length. Typically, the focal length ratio between T and W is greater than 3, while the f-stop ratio is less than 2.5. Therefore, the depth of field of T is usually significantly narrower than that of W. Thus, it is necessary to exclude defocused pixel areas using the defocus map to avoid introducing artifacts. Estimating the defocus map from a single image is a pathological problem that usually requires complex and computationally intensive machine learning models. To this end, we propose an efficient algorithm that reuses the optical flow information computed during the alignment stage to generate the defocus map:\nThe optical flow information contains depth and motion features of the image. By analyzing the relationship between optical flow and scene depth, we can efficiently generate the defocus map without additional complex models. Defocus Map Generation Method # To estimate the defocus map, we need to determine two key pieces of information:\nCamera\u0026rsquo;s Focus Position: the center area of the image that is in sharp focus. Relative Depth of Each Pixel with Respect to the Focus Position. Since the FOV of W and T is approximately parallel (fronto-parallel), and the magnitude of optical flow is proportional to the camera disparity, which is related to scene depth, we design an efficient optical flow-based defocus map estimation algorithm, as follows (see Figure 5): Obtain the Focus Region of Interest (ROI): Obtain the focus area from the camera\u0026rsquo;s auto-focus module. This module provides a rectangular area representing the region where most pixels in the T image are sharply focused (i.e., the focus ROI). Estimate the Focus Position \\(x_f\\) based on Optical Flow: Using dual-camera stereo vision, treat optical flow as a proxy for scene depth. Assuming that in a static scene, pixels located in the same focal plane have similar optical flow vectors (Szeliski 2022). Apply the k-means clustering algorithm to the optical flow vectors within the focus ROI to determine the area with the highest clustering density (i.e., the maximum cluster). The cluster center is defined as the focus position \\(x_f\\). Estimate Relative Depth of Pixels and Generate the Defocus Map: Calculate the Euclidean distance (L2 distance) between the optical flow vector of each pixel and the optical flow vector at the focus position \\(x_f\\). Defocus Map Calculation Formula # The calculation formula for the defocus map \\(M_{defocus}(x)\\) is as follows: \\( M_{defocus}(x) = \\text{sigmoid} \\left( \\frac{| F_{fwd}(x) - F_{fwd}(x_f) |_2^2 - \\gamma}{\\sigma_f} \\right) \\) where:\n\\(F_{fwd}(x)\\): the forward optical flow vector of pixel \\(x\\). \\(F_{fwd}(x_f)\\): the forward optical flow vector at the focus position \\(x_f\\). \\(\\gamma\\): a threshold that controls the tolerance range. \\(\\sigma_f\\): a parameter that controls the smoothness of the defocus map. Occlusion Map Calculation Formula # The occlusion map \\(M_{occ}(x)\\) is calculated based on forward-backward optical flow consistency, as follows: $$ M_{occ}(x) = \\min\\left(s \\cdot | W(W(x; F_{fwd}); F_{bwd}) - x |_2, 1 \\right) $$ where:\n\\(W\\): bilinear warping operator, used to map image coordinates \\(x\\) to new positions based on optical flow. \\(F_{fwd}\\): forward optical flow from the source image \\(I_{src}\\) to the reference image \\(I_{ref}\\). \\(F_{bwd}\\): backward optical flow from the reference image \\(I_{ref}\\) to the source image \\(I_{src}\\). \\(s\\): a scaling factor used to adjust the sensitivity of optical flow differences. \\(x\\): 2D image coordinates on the source image. Optical Flow Uncertainty Map # Due to the inherently ill-posed nature of dense correspondence problems, we enhance the functionality of PWC-Net to output an optical flow uncertainty map, which describes the uncertainty of each pixel\u0026rsquo;s optical flow prediction. The method is as follows:\nUncertainty Modeling:\nThe enhanced PWC-Net predicts a multivariate Laplacian distribution for the optical flow vector of each pixel, rather than a single point estimate. It predicts two additional channels representing the log-variance of optical flow in the x and y directions, denoted as \\({Var}_x\\) and \\({Var}_y\\).\nConvert to Pixel Units:\nThe log-variance is converted to pixel units using the following formulaâ€”here, \\(S(x)\\) is the optical flow uncertainty value for each pixel. \\( S(x) = \\sqrt{\\exp(\\log(\\text{Var}_x(x))) + \\exp(\\log(\\text{Var}_y(x)))} \\)\nGenerate Optical Flow Uncertainty Map:\n\\( M_{\\text{flow}}(x) = \\frac{\\min(S(x), s_{\\text{max}})}{s_{\\text{max}}} \\) The optical flow uncertainty map typically has higher values in object boundaries or texture-less regions.\nAlignment Rejection Map # To exclude artifacts introduced by alignment errors, we generate an alignment rejection map by comparing the local similarity between the source image and the aligned reference image. The method is as follows:\nMatch Optical Resolution: Use bilinear interpolation to adjust the resolution of the aligned reference frame \\(\\tilde{Y}{\\text{ref}}\\) to match the optical resolution of W, resulting in a downsampled image \\(\\tilde{Y}{\\text{ref}}^\\downarrow\\). Calculate Local Differences: For local patches \\(P_{\\text{src}}\\) from the source image and \\(P_{\\tilde{\\text{ref}}}\\) from the aligned reference image: Subtract the mean of the patches \\(u_{\\text{src}}\\) and \\(u_{\\text{ref}}\\). Calculate the normalized difference: $$ P_\\delta = (P_{\\text{src}} - \\mu_{\\text{src}}) - (P_{\\tilde{\\text{ref}}} - \\mu_{\\text{ref}}) $$ Generate the alignment rejection map $$ M_{\\text{reject}}(x) = 1 - \\exp\\left(-\\frac{|P_\\delta(x)|2^2}{\\sigma{\\text{src}}^2(x) + \\epsilon_0}\\right) $$ Final Blending # $$ M_{\\text{blend}} = \\max(1 - M_{\\text{occ}} - M_{\\text{defocus}} - M_{\\text{flow}} - M_{\\text{reject}}, 0) $$ The final output image is generated through alpha blending and cropped back to the full W image: $$ I_{\\text{final}} = \\text{uncrop}\\left(M_{\\text{blend}} \\odot I_{\\text{fusion}} + (1 - M_{\\text{blend}}) \\odot I_{\\text{src}}, W\\right) $$ By combining multiple masks (such as occlusion maps, defocus maps, optical flow uncertainty maps, and alignment rejection maps), we ensure high quality and robustness of the fusion results, avoiding the introduction of artifacts, blurriness, or alignment errors.\nImplementation Details # Results and Evaluation # Experiments: # Results # Comparisons # Critical Analysis # Strengths # Weaknesses # Insights # Broader Impact and Future Work # Broader Implications # Open Questions # Future Work # Personal Reflection # Relevance to My Work # Practical Applications # Key Learnings # Related Work and Context # Prior Work # Learning-based Single Image Super-Resolution (SISR)\nOver the past decade, various methods (e.g., Christian Ledig 2017; Dong et al. 2014; Kim et al. 2016; Lai et al. 2017; Wang et al. 2018; Xu et al. 2023; Zhang et al. 2019a, 2022b, 2018) have demonstrated outstanding results in the field of single image super-resolution. However, due to the severely ill-posed nature of this task, these methods tend to generate blurry details under large upsampling factors (e.g., 2-5 times, common in smartphone hybrid zoom). Additionally, some methods are only applicable to specific domains, such as face super-resolution (Chan et al. 2021; Gu et al. 2020; He et al. 2022; Menon et al. 2020).\nReference Super-Resolution (RefSR) Based on Internet Images\nRefSR generates high-resolution results from low-resolution inputs and one or more high-resolution reference images (Pesavento et al. 2021). Traditional RefSR methods assume that reference images come from the internet (Sun and Hays 2012) or are captured at different times, locations, or camera models during the same event (Wang et al. 2016; Zhang et al. 2019b), focusing on improving dense alignment between source and reference images (Huang et al. 2022; Jiang et al. 2021; Xia et al. 2022; Zheng et al. 2018) or enhancing robustness to unrelated reference images (Lu et al. 2021; Shim et al. 2020; Xie et al. 2020; Yang et al. 2020; Zhang et al. 2019b).\nIn contrast, we alleviate the alignment challenge by synchronously capturing W and T images, avoiding alignment issues caused by object motion.\nReference Super-Resolution (RefSR) Based on Auxiliary Cameras\nRecent RefSR studies (Trinidad et al. 2019; Wang et al. 2021; Zhang et al. 2022a) have captured reference images of the same scene using auxiliary cameras. However, due to the lack of pixel-aligned inputs and real label image pairs, PixelFusionNet (Trinidad et al. 2019) synthesizes low-resolution inputs from high-resolution reference images using degradation models and trains using pixel-level losses (e.g., l1 and VGG losses). However, this model performs poorly when faced with real scene inputs, primarily due to domain differences between training and inference stage images.\nOn the other hand, SelfDZSR (Zhang et al. 2022a), DCSR (Wang et al. 2021), and RefVSR (Lee et al. 2022) treat reference images as targets for training or fine-tuning. We observe that this training setup is prone to getting stuck in degraded local minima: the model often learns identity mappings, merely copying the content of T images to the output. This leads to severe alignment errors, color shifts, and depth of field mismatches, which are unacceptable in practical photography.\nTo address these issues, we additionally capture a T image during training to alleviate the aforementioned problems and improve training robustness.\nEfficient Mobile Device Reference Super-Resolution (RefSR)\nExisting methods often consume significant memory due to the use of attention mechanisms/Transformers (e.g., Wang et al. 2021; Yang et al. 2020) or deep network architectures (e.g., Zhang et al. 2022a). These methods may encounter out-of-memory (OOM) issues even on a desktop GPU with 40GB RAM when processing 12MP input resolutions, making them impractical for mobile devices. In contrast, our system processes 12MP inputs on mobile GPUs in just 500 milliseconds, using only 300MB of memory.\nOur system design is inspired by reference image deblurring methods for faces [Lai et al. 2022], but the problems we address are more challenging:\nSuper-Resolution for General Scenes: We apply super-resolution to general images rather than focusing solely on faces. Therefore, our system needs to be more robust to diverse scenes and capable of handling various imperfections and mismatches from both cameras. Domain Differences in Training Data: Unlike face deblurring models that can learn from synthetic data, image super-resolution models are more sensitive to domain differences in training data. Additionally, collecting real training data for reference-based super-resolution tasks is more challenging. Thus, our proposed adaptive blending method and dual smartphone platform design become key distinctions from the method in [Lai et al. 2022]. Historical Context # References # ","date":"2025å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/posts/ehz/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eAbstract \n    \u003cdiv id=\"abstract\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#abstract\" aria-label=\"ã‚¢ãƒ³ã‚«ãƒ¼\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eDSLR cameras can achieve various zoom levels by changing the lens distance or swapping lens types. However, due to space constraints, these techniques are not feasible on mobile devices. Most smartphones adopt a \u003cstrong\u003ehybrid zoom system\u003c/strong\u003e: typically using a wide-angle (W) camera for low zoom levels and a telephoto (T) camera for high zoom levels. To simulate zoom levels between W and T, these systems crop and digitally enlarge images from W, resulting in significant detail loss. In this paper, we propose an efficient hybrid zoom super-resolution system for mobile devices. This system captures \u003cstrong\u003esynchronized W and T image pairs\u003c/strong\u003e and \u003cstrong\u003eutilizes a machine learning model to align and transfer details from T to W\u003c/strong\u003e. We further develop an \u003cstrong\u003eadaptive blending method\u003c/strong\u003e that can handle \u003cstrong\u003edepth of field mismatches, scene occlusions, motion uncertainties, and alignment errors\u003c/strong\u003e. To minimize domain differences, we design a dual smartphone camera setup to capture real scene inputs and real label data for supervised training. In extensive evaluations on real scenes, our method can generate 12-megapixel images on mobile platforms in 500 milliseconds, achieving TA.\u003c/p\u003e","title":"Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes","type":"posts"},{"content":"","date":"2025å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":"","date":"2025å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2024å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/tags/%E3%82%BF%E3%82%B0/","section":"Tags","summary":"","title":"ã‚¿ã‚°","type":"tags"},{"content":" å‚è€ƒãƒªãƒ³ã‚¯ # ç§ã®PR opensearch-py-ml ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\nãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« Huggingface ä¸Šã®å…¬å¼ãƒ¢ãƒ‡ãƒ«ãƒšãƒ¼ã‚¸\nã¯ã˜ã‚ã« # AWS OpenSearchã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€å°‚ç”¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é€šã˜ã¦ç®¡ç†ã•ã‚Œã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‹ã‚‰æ¨è«–ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¾ã§ã®ã•ã¾ã–ã¾ãªæ¤œè¨¼ãŒè¡Œã‚ã‚Œã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯æŒ‡å®šã•ã‚ŒãŸã‚µãƒ¼ãƒãƒ¼ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒé©å®œæ›´æ–°ã•ã‚Œã¾ã™ã€‚\næ–°ã—ã„ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ãŸå¾Œã€å…ƒã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯åŠ¹æœçš„ã«æ¤œè¨¼ãŠã‚ˆã³ãƒ‡ãƒ—ãƒ­ã‚¤ã§ããªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯ã€å¾“æ¥ã®å¯†ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã„ãŸã‚ã§ã™ã€‚ãã®çµæœã€ç§ãŸã¡ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ç®¡ç†ãŠã‚ˆã³ç¶­æŒã™ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã«æ±ºã‚ã¾ã—ãŸã€‚\nç§ãŒè¡Œã£ãŸä½œæ¥­ # ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’è‡ªå‹•åŒ–ã—ã¾ã—ãŸï¼š ç§ã¯ï¼ˆã¾ãŸã¯ä¿®æ­£ã—ãŸï¼‰å®Œå…¨ãªGitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã€ç‰¹å®šã®æ¡ä»¶ï¼ˆmodel_type = Sparseã®ã¨ãï¼‰ã§ä»¥ä¸‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ãªPythonã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã¾ã—ãŸï¼š\nå…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚½ãƒ¼ã‚¹ã€IDã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã€ãƒ—ãƒ¼ãƒªãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã€èª¬æ˜ãªã©ï¼‰ è‡ªå‹•ãƒˆãƒ¬ãƒ¼ã‚¹ï¼šå…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’TorchScriptã¾ãŸã¯ONNXå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‹ã€ä¸¡æ–¹ã®å½¢å¼ã‚’åŒæ™‚ã«ç”Ÿæˆã—ã¾ã™ã€‚ åŸ‹ã‚è¾¼ã¿æ¤œè¨¼ï¼šå¤‰æ›ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã¨æ©Ÿèƒ½ãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚ ãƒ¢ãƒ‡ãƒ«ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèªï¼šå­˜åœ¨ã—ã€ä¸Šæ›¸ãã§ããªã„å ´åˆã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’æ‹’å¦ã—ã¾ã™ã€‚ æ‰‹å‹•æ‰¿èªï¼šé‡è¦ãªæ‹…å½“è€…ã®æ‰¿èªå¾Œã«ã®ã¿ã€æ­£å¼ã«S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨è¨˜éŒ²ã‚’æ›´æ–°ï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’MODEL_UPLOAD_HISTORY.mdã€supported_models.jsonã«è¿½åŠ ã—ã€CHANGELOG.mdã‚’æ›´æ–°ã—ã¾ã™ã€‚ ä¸‹æµã®Jenkinsãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒªãƒªãƒ¼ã‚¹ã¾ãŸã¯ml-modelsã¸ã®çµ±åˆã‚’ã•ã‚‰ã«å®Œäº†ã—ã¾ã™ã€‚ ä½¿ç”¨æŠ€è¡“ # Python GitHub Actions Huggingface Jenkins AWS S3 AWS OpenSearch AWS IAM AWS Secrets Manager ","date":"2024å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/projects/1736923185327-a-model-release-pipeline/","section":"Projects","summary":"ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ãƒªãƒªãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³","title":"ãƒ¢ãƒ‡ãƒ«ãƒªãƒªãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³","type":"projects"},{"content":"","date":"2024å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/tags/%E4%BE%8B/","section":"Tags","summary":"","title":"ä¾‹","type":"tags"},{"content":"","date":"2023å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS","type":"tags"},{"content":" å‚è€ƒãƒªãƒ³ã‚¯ # RFC by me\nPR by me\nBlog post by me\nã¯ã˜ã‚ã« # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã¯ã€OpenSearch 2.11ã§å°å…¥ã•ã‚ŒãŸæ„å‘³çš„æ¤œç´¢ã®åŠ¹ç‡çš„ãªæ–¹æ³•ã§ã™ã€‚ã“ã‚Œã¯ã€æ„å‘³çš„æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã‚’è§£é‡ˆã—ã€å¾“æ¥ã®æ¤œç´¢ã§ã¯è¦‹é€ƒã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ç”¨èªã‚’å‡¦ç†ã—ã¾ã™ã€‚å¯†ãªãƒ¢ãƒ‡ãƒ«ã¯é¡ä¼¼ã®çµæœã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€æ­£ç¢ºãªä¸€è‡´ã‚’è¦‹é€ƒã™ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã¯ã€æ„å‘³çš„é¡ä¼¼æ€§ã¨ç‰¹å®šã®ç”¨èªã®ä¸¡æ–¹ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾ã‚’ä½¿ç”¨ã—ã€ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªæ¤œç´¢ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã™ã‚‹ã“ã¨ã§çµæœã®èª¬æ˜ã¨æç¤ºã‚’æ”¹å–„ã—ã¾ã™ã€‚\nèƒŒæ™¯ # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã¯ã€æœ€åˆã«ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¯ã‚¨ãƒªã¾ãŸã¯æ–‡æ›¸ã®ã„ãšã‚Œã‹ï¼‰ã‚’ã‚ˆã‚Šå¤§ããªç”¨èªã®ã‚»ãƒƒãƒˆã«æ‹¡å¼µã—ã€ãã‚Œãã‚Œã®æ„å‘³çš„é–¢é€£æ€§ã«åŸºã¥ã„ã¦é‡ã¿ä»˜ã‘ã‚’è¡Œã„ã¾ã™ã€‚æ¬¡ã«ã€Luceneã®åŠ¹ç‡çš„ãªç”¨èªãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¦ã€æœ€é«˜å¾—ç‚¹ã®çµæœã‚’ç‰¹å®šã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆã®å‰Šæ¸›ã€ãªã‚‰ã³ã«è¨ˆç®—ã‚³ã‚¹ãƒˆã®ä½ä¸‹ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚ãŸã¨ãˆã°ã€k-NNæ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå¯†ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ã€æ¤œç´¢æ™‚ã«RAMã‚³ã‚¹ãƒˆã‚’7.9%å¢—åŠ ã•ã›ã¾ã™ãŒã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã¯ãƒã‚¤ãƒ†ã‚£ãƒ–ã®Luceneã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã€æ¤œç´¢æ™‚ã®RAMã‚³ã‚¹ãƒˆã®å¢—åŠ ã‚’å›é¿ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã¯ã€å¯†ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã¯ã‚‹ã‹ã«å°ã•ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚æ–‡æ›¸å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã¯ã€å¯†ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚ãšã‹10.4%ã®ã‚µã‚¤ã‚ºã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã—ã€ãƒã‚¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å ´åˆã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã¯å¯†ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®7.2%ã§ã™ã€‚\nã“ã‚Œã‚‰ã®åˆ©ç‚¹ã‚’è€ƒæ…®ã—ã¦ã€ç§ãŸã¡ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã‚’ã•ã‚‰ã«åŠ¹ç‡çš„ã«ã™ã‚‹ãŸã‚ã«æ”¹è‰¯ã‚’ç¶šã‘ã¦ã„ã¾ã™ã€‚OpenSearch 2.15ã§ã¯ã€æ–°ã—ã„æ©Ÿèƒ½ã¨ã—ã¦äºŒç›¸æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå°å…¥ã•ã‚Œã¾ã—ãŸã€‚ã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªç”¨èªã‚’2ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†å‰²ã—ã¾ã™ï¼šæ¤œç´¢ã«ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„é«˜å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã€é–¢é€£æ€§ã®ä½ã„ä½å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã§ã™ã€‚æœ€åˆã«ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯é«˜å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦æ–‡æ›¸ã‚’é¸æŠã—ã€ãã®å¾Œã€é«˜å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä½å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸¡æ–¹ã‚’å«ã‚ã¦ãã‚Œã‚‰ã®æ–‡æ›¸ã®ã‚¹ã‚³ã‚¢ã‚’å†è¨ˆç®—ã—ã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€æœ€çµ‚çš„ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®è³ªã‚’ç¶­æŒã—ãªãŒã‚‰ã€è¨ˆç®—è² è·ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚\näºŒç›¸ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  # äºŒç›¸æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€2ã¤ã®æ®µéšã§å‹•ä½œã—ã¾ã™ï¼š\nåˆæœŸæ®µéš # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’ä½¿ç”¨ã—ã¦ã€ã‚¯ã‚¨ãƒªã‹ã‚‰ã®é«˜å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦å€™è£œæ–‡æ›¸ã®ã‚»ãƒƒãƒˆã‚’è¿…é€Ÿã«é¸æŠã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®é«˜å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸­ã§å°ã•ãªéƒ¨åˆ†ã‚’æ§‹æˆã—ã€é‡è¦ãªé‡ã¿ã¾ãŸã¯é–¢é€£æ€§ã‚’æŒã¡ã€æ½œåœ¨çš„ã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’è¿…é€Ÿã«ç‰¹å®šã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€å‡¦ç†ã™ã‚‹å¿…è¦ã®ã‚ã‚‹æ–‡æ›¸ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ä½ä¸‹ã•ã›ã¾ã™ã€‚\nå†è¨ˆç®—æ®µéš # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€æœ€åˆã®æ®µéšã§é¸æŠã•ã‚ŒãŸå€™è£œæ–‡æ›¸ã®ã‚¹ã‚³ã‚¢ã‚’å†è¨ˆç®—ã—ã¾ã™ã€‚ã“ã®ã¨ãã€ã‚¯ã‚¨ãƒªã‹ã‚‰ã®é«˜å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä½å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸¡æ–¹ã‚’å«ã‚ã¾ã™ã€‚ä½å¾—ç‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¯å€‹åˆ¥ã«ã¯é‡ã¿ãŒå°‘ãªã„ã§ã™ãŒã€åŒ…æ‹¬çš„ãªè©•ä¾¡ã®ä¸€éƒ¨ã¨ã—ã¦è²´é‡ãªæƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚ç‰¹ã«ã€ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ç”¨èªãŒå…¨ä½“ã®ã‚¹ã‚³ã‚¢ã«å¤§ããå¯„ä¸ã™ã‚‹å ´åˆã€ã“ã‚Œã«ã‚ˆã‚Šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æœ€çµ‚çš„ãªæ–‡æ›¸ã‚¹ã‚³ã‚¢ã‚’ã‚ˆã‚Šæ­£ç¢ºã«æ±ºå®šã§ãã¾ã™ã€‚\næ–‡æ›¸ã‚’æ®µéšçš„ã«å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã—ã€ç²¾åº¦ã‚’ç¶­æŒã—ã¾ã™ã€‚æœ€åˆã®æ®µéšã§ã®è¿…é€Ÿãªé¸æŠã¯åŠ¹ç‡ã‚’é«˜ã‚ã€2ç•ªç›®ã®æ®µéšã§ã®è©³ç´°ãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¯ç²¾åº¦ã‚’ç¢ºä¿ã—ã¾ã™ã€‚å¤šãã®ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ç”¨èªã‚’å‡¦ç†ã—ã¦ã‚‚ã€çµæœã¯é«˜å“è³ªã®ã¾ã¾ã§ã‚ã‚Šã€è¨ˆç®—åŠ¹ç‡ãŒè‘—ã—ãæ”¹å–„ã•ã‚Œã¾ã™ã€‚\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«å¿œã˜ã¦ã€äºŒç›¸ãƒ—ãƒ­ã‚»ãƒƒã‚µã¯æ–‡æ›¸å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã§1.22å€ã‹ã‚‰1.78å€ã®é€Ÿåº¦å‘ä¸Šã‚’é”æˆã—ã¾ã—ãŸã€‚ ![two-phase-doc-model-p99-latency]](doc-only.png )\nã¾ãŸã€ãƒã‚¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¢ãƒ‡ãƒ«ã§4.15å€ã‹ã‚‰6.87å€ã®é€Ÿåº¦å‘ä¸Šã‚’é”æˆã—ã¾ã—ãŸã€‚ End of Selection # ","date":"2023å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/projects/1736922318122-neural-sparse-search-on-aws-opensearch/","section":"Projects","summary":"AWS OpenSearchã«ãŠã‘ã‚‹äºŒç›¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢","title":"AWS OpenSearchã«ãŠã‘ã‚‹äºŒç›¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢","type":"projects"},{"content":"","date":"2023å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/tags/neural-search/","section":"Tags","summary":"","title":"Neural Search","type":"tags"},{"content":"","date":"2023å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/tags/opensearch/","section":"Tags","summary":"","title":"OpenSearch","type":"tags"},{"content":"","date":"2023å¹´1æœˆ15æ—¥","externalUrl":null,"permalink":"/tags/two-phase-search/","section":"Tags","summary":"","title":"Two Phase Search","type":"tags"},{"content":"","date":"2022å¹´1æœˆ4æ—¥","externalUrl":null,"permalink":"/tags/c++/","section":"Tags","summary":"","title":"C++","type":"tags"},{"content":" D5 Render # D5 Renderã¯ã€å»ºç¯‰å®¶ã€ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã€ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®å°‚é–€å®¶ã«ã€ãƒ•ã‚©ãƒˆãƒªã‚¢ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã§åŠ¹ç‡çš„ãªãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã™ã‚‹æœ€å…ˆç«¯ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ 3Dãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã™ã€‚ãã®ç›´æ„Ÿçš„ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€é«˜åº¦ãªãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ã€ãã—ã¦å¤šæ§˜ãªæ©Ÿèƒ½ã¯ã€æœ€å°é™ã®å­¦ç¿’æ›²ç·šã§é«˜å“è³ªãªçµæœã‚’æ±‚ã‚ã‚‹å°‚é–€å®¶ã«ã¨ã£ã¦å„ªã‚ŒãŸé¸æŠè‚¢ã¨ãªã‚Šã¾ã™ã€‚\nUser Case # ã»ã¨ã‚“ã©ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€ã•ã¾ã–ã¾ãª3Dãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’ä½¿ç”¨ã—ã¦3Dãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ä¿å­˜ã—ã€ãã®å¾ŒD5 Renderã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€ç…§æ˜ã®ãƒªã‚¢ãƒ«ãªåŠ¹æœã‚’è¦³å¯Ÿã—ã¾ã™ã€‚å»ºç¯‰æ¥­ç•Œã§ã¯ã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®é »ç¹ãªä¿®æ­£ãŒå¿…è¦ã¨ã•ã‚Œã‚‹ã“ã¨ãŒå¤šãã€ä¾‹ãˆã°ã€æ™¯è¦³è¦ç´ ã‚’å‰Šé™¤ã—ãŸã‚Šã€ç‰¹å®šã®æ©Ÿèƒ½ã‚’ã‚µã‚¤ã‚ºå¤‰æ›´ã—ãŸã‚Šã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¨3Dãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®é–“ã‚’å¸¸ã«åˆ‡ã‚Šæ›¿ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€é‡è¦ãªç—›ç‚¹ãŒç”Ÿã˜ã¾ã™ã€‚\nSolution # ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã¯D5 Renderç”¨ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚ã“ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯Rhinoã§3Dãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ç·¨é›†ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ã“ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯Rhinoã§3Dãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ã‚’åŠ ãˆã€2ã¤ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®é–“ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãªãã€D5 Renderã§å³åº§ã«çµæœã‚’ç¢ºèªã§ãã¾ã™ã€‚\nVideo # My Role # ç§ã¯ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®å…¨ä½“çš„ãªè¨­è¨ˆã¨å®Ÿè£…ã‚’æ‹…å½“ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã¯ã€ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ã‚³ã‚¢æ©Ÿèƒ½ã®é–‹ç™ºã€D5 Renderã¨ã®çµ±åˆã€ãŠã‚ˆã³ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ†ã‚¹ãƒˆã¨æ¤œè¨¼ãŒå«ã¾ã‚Œã¾ã™ã€‚\nTechnology # C++ RhinoSDK D5 Render Result # ã‚¼ãƒ­ã‹ã‚‰ã€ç§ã¯ã“ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ãƒ™ãƒ¼ã‚¿ç‰ˆã‚’2ãƒ¶æœˆã§æ§‹ç¯‰ã—ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ãã—ã¦ä»Šã€å¤šãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚\n","date":"2022å¹´1æœˆ4æ—¥","externalUrl":null,"permalink":"/projects/1735963425218-d5-render-rhino-sync-plugin/","section":"Projects","summary":"D5 Renderã¨Rhinoã®åŒæœŸãƒ—ãƒ©ã‚°ã‚¤ãƒ³","title":"D5 Render-Rhino åŒæœŸãƒ—ãƒ©ã‚°ã‚¤ãƒ³","type":"projects"},{"content":" ğŸ“ è‡ªå·±ç´¹ä»‹ # ç§ã¯ä¸­å›½å±±æ±çœã§ç”Ÿã¾ã‚Œã€å¤©æ´¥å¸‚ã§è‚²ã¡ã¾ã—ãŸã€‚å—äº¬ã§7å¹´é–“å­¦ã‚“ã å¾Œã€åšå£«èª²ç¨‹ã‚’ç›®æŒ‡ã—ã¦æ—¥æœ¬ã«æ¥ã¾ã—ãŸã€‚\næ—¥æœ¬ã®ã‚«ãƒ¬ãƒ¼ã¨ã™ãç„¼ããŒå¤§å¥½ãã§ã™ã€‚è¶£å‘³ã¯ã‚µãƒƒã‚«ãƒ¼ã¨æ—¥æœ¬ã®ãƒ­ãƒƒã‚¯éŸ³æ¥½ã‚’è´ãã“ã¨ã§ã€ç‰¹ã«ãƒ–ãƒ«ãƒ¼ãƒãƒ¼ãƒ„ã¨X JAPANãŒå¤§å¥½ããªãƒãƒ³ãƒ‰ã§ã™ã€‚\nå­¦æ­´ # æœŸé–“ æ‰€åœ¨åœ° æ©Ÿé–¢ å½¹è·ãƒ»å°‚æ”» ä¸»ãªæ´»å‹•å†…å®¹ 2016-2020 ä¸­å›½ æ±å—å¤§å­¦ ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ å­¦å£« ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãªã©ã‚’å­¦ã¶ã€‚ 2020-2023 ä¸­å›½ å—äº¬å¤§å­¦ ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ ä¿®å£« ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’å¿œç”¨ã‚’ç ”ç©¶ã€‚ 2023-2024 ä¸­å›½ AWS ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ AWS OpenSearch ã‚µãƒ¼ãƒ“ã‚¹ã®é–‹ç™ºã¨ä¿å®ˆã‚’æ‹…å½“ã€‚ 2023-ç¾åœ¨ æ—¥æœ¬ æ—©ç¨²ç”°å¤§å­¦ ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ åšå£« ç­‰ç ”ç©¶ã‚’é€²ã‚ã‚‹ã€‚ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³çµŒæ­´ # æœŸé–“ æ‰€åœ¨åœ° æ©Ÿé–¢ å½¹è·ãƒ»ãƒ†ãƒ¼ãƒ ä¸»ãªæ´»å‹•å†…å®¹ 2025.9 æ—¥æœ¬ ã‚½ãƒ‹ãƒ¼ã‚»ãƒŸã‚³ãƒ³ãƒ€ã‚¯ã‚¿ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚ºã‚°ãƒ«ãƒ¼ãƒ— R\u0026amp;Dã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ - 3é€±é–“ã®è·å ´å¯†ç€å‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã‚·ãƒƒãƒ—ã«å‚åŠ ã—ã€ã‚«ãƒ¡ãƒ©ISPã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãŠã‘ã‚‹ãƒ‡ãƒ¢ã‚¶ã‚¤ã‚­ãƒ³ã‚°ãŠã‚ˆã³ãƒã‚¤ã‚ºé™¤å»ã«é–¢ã™ã‚‹å®Ÿé¨“çš„æ¤œè¨ã‚’è¡Œã„ã¾ã—ãŸã€‚ - å¯å¤‰ãƒã‚¤ã‚ºç’°å¢ƒã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆãªå­¦ç¿’æ‰‹æ³•ã‚’æ¢ç´¢ã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Šã®å¯èƒ½æ€§ã‚’æ¤œè¨ã€‚ - ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã€RAWã‹ã‚‰SR RGBã¸ã®ãƒ¯ãƒ³ã‚¹ãƒ†ãƒƒãƒ—å¤‰æ›ã‚’è©¦ã¿ã€ãƒ‡ãƒ¢ã‚¶ã‚¤ã‚­ãƒ³ã‚°ãƒ»ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…è§£åƒã‚’çµ±åˆçš„ã«å®Ÿç¾ã™ã‚‹æ–¹å‘æ€§ã‚’æ¢ç´¢ã€‚ 2025.5 â€“ 2025.8 ä¸­å›½ ã‚½ãƒ‹ãƒ¼R\u0026amp;Dã‚»ãƒ³ã‚¿ãƒ¼ China Laboratory R\u0026amp;Dã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ - ç”»åƒç”ŸæˆæŠ€è¡“ã«åŸºã¥ãç”»åƒãƒ»å‹•ç”»ã®è¶…è§£åƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç ”ç©¶ã€‚\n- ãƒ•ãƒ­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ãŠã‚ˆã³æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å…ˆè¡Œç ”ç©¶ã‚’èª¿æŸ»ã—ã€å®Ÿé¨“ã‚’é€šã˜ã¦çŸ¥è¦‹ã‚’è“„ç©ã€‚\n- ç²¾åº¦ãƒ»é€Ÿåº¦ãƒ»é©ç”¨æ€§ã‚’åˆ†æã—ã€ç ”ç©¶ãƒ†ãƒ¼ãƒã®æ–¹å‘æ€§æ±ºå®šã«è²¢çŒ®ã€‚ 2023.9 â€“ 2024.9 ä¸­å›½ ã‚¢ãƒã‚¾ãƒ³ãƒ»ã‚¦ã‚§ãƒ–ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆAWSï¼‰ æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ - ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«åŸºã¥ã„ãŸæ„å‘³åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã—ã€GPUä¸è¦ã®ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚’å®Ÿç¾ã€‚\n- æ¤œç´¢é€Ÿåº¦ã‚’5-8å€é«˜é€ŸåŒ–ã™ã‚‹äºŒæ®µéšæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã€‚\n- CI/CDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«é…ä¿¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã€‚\n- Luceneã®ã‚«ãƒ©ãƒ ãƒŠã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãŠã‘ã‚‹é‡å­åŒ–ã¨ãƒ“ãƒƒãƒˆã‚»ãƒƒãƒˆæœ€é©åŒ–ã‚’å®Ÿæ–½ã€‚ 2021.6 â€“ 2021.9 ä¸­å›½ D5 Render 3Dã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ - RhinoCDKã‚’ç”¨ã„ã¦Rhinoä¸Šã®å»ºç¯‰ãƒ¡ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åŒæœŸã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã€‚\n- ã‚¸ã‚ªãƒ¡ãƒˆãƒªã€ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã€ãƒãƒ†ãƒªã‚¢ãƒ«å¤‰æ›´ã‚’å«ã‚€ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å‡¦ç†ã«å¯¾å¿œã€‚\n- å•†ç”¨ã‚¢ãƒ—ãƒªã¨ã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«æ¡ç”¨ã•ã‚Œã‚‹å …ç‰¢ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã€‚ ","externalUrl":null,"permalink":"/about/","section":"","summary":"\u003ch2 class=\"relative group\"\u003eğŸ“ è‡ªå·±ç´¹ä»‹ \n    \u003cdiv id=\"-è‡ªå·±ç´¹ä»‹\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#-%e8%87%aa%e5%b7%b1%e7%b4%b9%e4%bb%8b\" aria-label=\"ã‚¢ãƒ³ã‚«ãƒ¼\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eç§ã¯ä¸­å›½\u003cstrong\u003eå±±æ±çœ\u003c/strong\u003eã§ç”Ÿã¾ã‚Œã€\u003cstrong\u003eå¤©æ´¥å¸‚\u003c/strong\u003eã§è‚²ã¡ã¾ã—ãŸã€‚å—äº¬ã§7å¹´é–“å­¦ã‚“ã å¾Œã€åšå£«èª²ç¨‹ã‚’ç›®æŒ‡ã—ã¦\u003cstrong\u003eæ—¥æœ¬\u003c/strong\u003eã«æ¥ã¾ã—ãŸã€‚\u003c/p\u003e\n\u003cp\u003eæ—¥æœ¬ã®\u003cstrong\u003eã‚«ãƒ¬ãƒ¼\u003c/strong\u003eã¨\u003cstrong\u003eã™ãç„¼ã\u003c/strong\u003eãŒå¤§å¥½ãã§ã™ã€‚è¶£å‘³ã¯\u003cstrong\u003eã‚µãƒƒã‚«ãƒ¼\u003c/strong\u003eã¨\u003cstrong\u003eæ—¥æœ¬ã®ãƒ­ãƒƒã‚¯éŸ³æ¥½\u003c/strong\u003eã‚’è´ãã“ã¨ã§ã€ç‰¹ã«\u003cstrong\u003eãƒ–ãƒ«ãƒ¼ãƒãƒ¼ãƒ„\u003c/strong\u003eã¨\u003cstrong\u003eX JAPAN\u003c/strong\u003eãŒå¤§å¥½ããªãƒãƒ³ãƒ‰ã§ã™ã€‚\u003c/p\u003e\n\u003chr\u003e\n\n\u003ch2 class=\"relative group\"\u003eå­¦æ­´ \n    \u003cdiv id=\"å­¦æ­´\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#%e5%ad%a6%e6%ad%b4\" aria-label=\"ã‚¢ãƒ³ã‚«ãƒ¼\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003eæœŸé–“\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eæ‰€åœ¨åœ°\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eæ©Ÿé–¢\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eå½¹è·ãƒ»å°‚æ”»\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eä¸»ãªæ´»å‹•å†…å®¹\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2016-2020\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eä¸­å›½\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eæ±å—å¤§å­¦\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ å­¦å£«\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãªã©ã‚’å­¦ã¶ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2020-2023\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eä¸­å›½\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eå—äº¬å¤§å­¦\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ ä¿®å£«\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’å¿œç”¨ã‚’ç ”ç©¶ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2023-2024\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eä¸­å›½\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eAWS\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eAWS OpenSearch ã‚µãƒ¼ãƒ“ã‚¹\u003c/strong\u003eã®é–‹ç™ºã¨ä¿å®ˆã‚’æ‹…å½“ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2023-ç¾åœ¨\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eæ—¥æœ¬\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eæ—©ç¨²ç”°å¤§å­¦\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ åšå£«\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eç­‰ç ”ç©¶ã‚’é€²ã‚ã‚‹ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\n\u003ch2 class=\"relative group\"\u003eã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³çµŒæ­´ \n    \u003cdiv id=\"ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³çµŒæ­´\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#%e3%82%a4%e3%83%b3%e3%82%bf%e3%83%bc%e3%83%b3%e7%b5%8c%e6%ad%b4\" aria-label=\"ã‚¢ãƒ³ã‚«ãƒ¼\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003eæœŸé–“\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eæ‰€åœ¨åœ°\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eæ©Ÿé–¢\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eå½¹è·ãƒ»ãƒ†ãƒ¼ãƒ\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eä¸»ãªæ´»å‹•å†…å®¹\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2025.9\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eæ—¥æœ¬\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eã‚½ãƒ‹ãƒ¼ã‚»ãƒŸã‚³ãƒ³ãƒ€ã‚¯ã‚¿ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚ºã‚°ãƒ«ãƒ¼ãƒ—\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eR\u0026amp;Dã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e- 3é€±é–“ã®è·å ´å¯†ç€å‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã‚·ãƒƒãƒ—ã«å‚åŠ ã—ã€ã‚«ãƒ¡ãƒ©ISPã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãŠã‘ã‚‹ãƒ‡ãƒ¢ã‚¶ã‚¤ã‚­ãƒ³ã‚°ãŠã‚ˆã³ãƒã‚¤ã‚ºé™¤å»ã«é–¢ã™ã‚‹å®Ÿé¨“çš„æ¤œè¨ã‚’è¡Œã„ã¾ã—ãŸã€‚  \u003cbr\u003e- å¯å¤‰ãƒã‚¤ã‚ºç’°å¢ƒã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆãªå­¦ç¿’æ‰‹æ³•ã‚’æ¢ç´¢ã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Šã®å¯èƒ½æ€§ã‚’æ¤œè¨ã€‚ \u003cbr\u003e- ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã€RAWã‹ã‚‰SR RGBã¸ã®ãƒ¯ãƒ³ã‚¹ãƒ†ãƒƒãƒ—å¤‰æ›ã‚’è©¦ã¿ã€ãƒ‡ãƒ¢ã‚¶ã‚¤ã‚­ãƒ³ã‚°ãƒ»ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…è§£åƒã‚’çµ±åˆçš„ã«å®Ÿç¾ã™ã‚‹æ–¹å‘æ€§ã‚’æ¢ç´¢ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2025.5 â€“ 2025.8\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eä¸­å›½\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eã‚½ãƒ‹ãƒ¼R\u0026amp;Dã‚»ãƒ³ã‚¿ãƒ¼ China Laboratory\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eR\u0026amp;Dã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e- ç”»åƒç”ŸæˆæŠ€è¡“ã«åŸºã¥ãç”»åƒãƒ»å‹•ç”»ã®è¶…è§£åƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç ”ç©¶ã€‚\u003cbr\u003e- ãƒ•ãƒ­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ãŠã‚ˆã³æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å…ˆè¡Œç ”ç©¶ã‚’èª¿æŸ»ã—ã€å®Ÿé¨“ã‚’é€šã˜ã¦çŸ¥è¦‹ã‚’è“„ç©ã€‚\u003cbr\u003e- ç²¾åº¦ãƒ»é€Ÿåº¦ãƒ»é©ç”¨æ€§ã‚’åˆ†æã—ã€ç ”ç©¶ãƒ†ãƒ¼ãƒã®æ–¹å‘æ€§æ±ºå®šã«è²¢çŒ®ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2023.9 â€“ 2024.9\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eä¸­å›½\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eã‚¢ãƒã‚¾ãƒ³ãƒ»ã‚¦ã‚§ãƒ–ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆAWSï¼‰\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eæ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e- ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«åŸºã¥ã„ãŸæ„å‘³åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã—ã€GPUä¸è¦ã®ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚’å®Ÿç¾ã€‚\u003cbr\u003e- æ¤œç´¢é€Ÿåº¦ã‚’5-8å€é«˜é€ŸåŒ–ã™ã‚‹äºŒæ®µéšæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã€‚\u003cbr\u003e- CI/CDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«é…ä¿¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã€‚\u003cbr\u003e- Luceneã®ã‚«ãƒ©ãƒ ãƒŠã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãŠã‘ã‚‹é‡å­åŒ–ã¨ãƒ“ãƒƒãƒˆã‚»ãƒƒãƒˆæœ€é©åŒ–ã‚’å®Ÿæ–½ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e2021.6 â€“ 2021.9\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eä¸­å›½\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eD5 Render\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e3Dã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e- RhinoCDKã‚’ç”¨ã„ã¦Rhinoä¸Šã®å»ºç¯‰ãƒ¡ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åŒæœŸã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ã€‚\u003cbr\u003e- ã‚¸ã‚ªãƒ¡ãƒˆãƒªã€ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã€ãƒãƒ†ãƒªã‚¢ãƒ«å¤‰æ›´ã‚’å«ã‚€ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å‡¦ç†ã«å¯¾å¿œã€‚\u003cbr\u003e- å•†ç”¨ã‚¢ãƒ—ãƒªã¨ã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«æ¡ç”¨ã•ã‚Œã‚‹å …ç‰¢ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã€‚\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e","title":"","type":"about"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯PDFã®åŸ‹ã‚è¾¼ã¿ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚ ã“ã¡ã‚‰ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚ ","externalUrl":null,"permalink":"/resume/","section":"ã‹ã‚“ãã†","summary":"\u003ciframe \n    src=\"/resume/congguan_jp.pdf\" \n    width=\"200%\" \n    height=\"800px\" \n\u003e\nãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯PDFã®åŸ‹ã‚è¾¼ã¿ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚\n\u003ca href=\"/resume/congguan_jp.pdf\"\u003eã“ã¡ã‚‰ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\u003c/a\u003eã—ã¦ãã ã•ã„ã€‚\n\u003c/iframe\u003e","title":"Kansouã®å±¥æ­´æ›¸","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]