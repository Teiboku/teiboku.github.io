[{"content":" Abstract # DSLR 相机可以通过改变镜头距离或更换镜头类型实现多种变焦级别。然而，由于空间限制，这些技术在智能手机设备上不可行。大多数手机采用混合变焦系统：通常使用低变焦级别的广角（W）摄像头和高变焦级别的长焦（T）摄像头。 为了模拟介于 W 和 T 之间的变焦级别，这些系统会对来自 W 的图像进行裁剪和数字放大，导致细节损失显著。在本文中，我们提出了一种高效的移动设备混合变焦超分辨率系统。该系统捕获同步的 W 和 T 图像对，并利用机器学习模型将 T 的细节对齐并传递到 W。我们进一步开发了一种自适应混合方法，该方法可以处理景深不匹配、场景遮挡、流动不确定性以及对齐误差。为了尽量减少域差异，我们设计了一个双手机摄像头装置，用于捕捉真实场景输入和用于监督训练的真实标签数据。在真实场景的广泛评估中，我们的方法可以在移动平台上生成 12 百万像素图像，耗时 500 毫秒，并达到了TA.\nKey word # hybrid zoom, dual camera fusion, deep neural networks\nKey Takeaways # Problem Addressed # 几乎所有的上采样方法（如双线性插值、双三次插值等）都会导致不同程度的画质下降, 因此, 涌现了众多利用高变焦 T 图像作为参考，为低变焦 W 图像添加真实细节的方法. 商业解决方案并未公开, 学术研究上效率较低. 。这些方法通常在移动设备上效率较低，容易受到参考图像缺陷的影响，并可能在训练和推理之间引入域差异。 对以上问题.提出了一种混合变焦超分辨率（Hybrid Zoom Super-Resolution, HZSR）系统以加以解决。\nMain Contributions # 基于机器学习的混合变焦超分辨率（HZSR）系统，该系统在移动设备上高效运行，并且对真实场景图像中的不完美情况具有较强的鲁棒性（详见第 3 节）。 一种训练策略，通过双手机摄像头平台最小化域差异，并避免在参考超分辨率（RefSR）任务中学习到平凡映射（详见第 4 节）。 一个包含 150 组高分辨率（12MP）的 W 和 T 图像对的高质量同步数据集，命名为 Hzsr 数据集，将在我们的项目网站上发布，用于未来研究（详见第 5 节）。 Methodology # Approach # 适应不完美的参考图像 # 提出了一种高效的失焦检测算法，该算法基于场景深度和光流之间的相关性来排除失焦区域。结合失焦图、对齐误差、光流不确定性和场景遮挡信息，我们设计了一种自适应混合机制，以生成高质量、无伪影的超分辨率结果。\n通过真实场景输入最小化域差异 # 在参考超分辨率（RefSR）任务中，收集完全对齐的 W/T 图像对作为训练数据是非常困难的。因此，以往研究探索了两种可能但不理想的解决方案：\n将参考图像 T 用作训练目标（如 Wang et al. 2021 和 Zhang et al. 2022a），这种方法可能会传递参考图像的缺陷或导致网络学习恒等映射（identity mapping）。 通过退化模型从目标图像合成低分辨率输入（如 Trinidad et al. 2019 和 Zhang et al. 2022a），但此方法在训练和推理阶段之间引入了域差异，进而降低了对真实场景图像的超分辨率效果。 为了避免学习恒等映射并最小化域差异，我们采用了一种设计：在训练过程中，使用安装在摄影平台上的第二部相同型号的智能手机同步捕捉额外的 T 图像作为参考（见图 6）。 通过这种设计，融合模型在训练和推理阶段均以真实的 W 图像作为输入，从而消除域差异。此外，参考图像和目标图像由不同设备的 T 摄像头捕获，以避免网络学习恒等映射。 与现有双变焦 RefSR 数据集相比，我们的设计具备显著优势：\n一些数据集存在 W 和 T 之间的强时间运动（如 Wang et al. 2021）； 一些数据集仅限于静态场景（如 Wei et al. 2020）。 我们收集了一个大规模数据集，其中包含高质量的 W/T 同步数据，覆盖了动态场景，包括人像、建筑、风景，以及动态物体运动和夜间场景等具有挑战性的场景。实验表明，我们的方法在现有双变焦 RefSR 数据集和我们的新数据集上均优于当前最先进的方法。 Model/Algorithm # 当用户将变焦调整至中等范围（例如 3-5 倍）并按下快门按钮时，系统会捕获一对同步的图像。处理流程如下：\n图像对齐（Alignment）： # 首先，通过关键点匹配进行全局粗对齐，随后使用光流执行局部密集对齐（详见第 3.1 节）。 粗对齐（Coarse Alignment）：\n首先，我们对广角摄像头的图像（W）进行裁剪，使其视场角（FOV）与长焦摄像头的图像（T）一致。接着，使用双三次插值将 W 的空间分辨率调整到与 T 匹配（4k×3k）。随后，通过 FAST 特征点匹配算法（Rosten and Drummond 2006）估计全局二维平移向量，并调整裁剪后的 W，得到调整后的图像 IsrcI_{src}Isrc​ 密集对齐（Dense Alignment）：\n我们使用 PWC-Net估计 $I_{src}​$ 和 $I_{ref}$​ 之间的稠密光流。需要注意的是，W 和 T 之间在 12MP 分辨率下的平均偏移量约为 150 像素，这远大于大多数光流训练数据中的运动幅度。因此，从 12MP 图像估计的光流通常会非常嘈杂。 为了提高精度，我们将 $I_{src}$​ 和 $I_{ref}$​ 下采样到 384×512 的较小分辨率下预测光流，然后将光流上采样到原始分辨率，并通过双线性重采样将 $I_{ref}$​ 变形得到 $\\tilde{I}{ref}$。这种方法在较小尺度下估计的光流更准确且更具鲁棒性。 优化 为了适应移动设备的计算资源限制，我们移除了原始 PWC-Net 中的 DenseNet 结构，减少了模型大小 50%，延迟 56%，以及峰值内存 63%。尽管在 Sintel 数据集上的光流端点误差（EPE）增加了 8%，但光流的视觉质量依然保持相似。\n此外，我们通过前向-后向一致性检查（forward-backward consistency check, Alvarez et al. 2007）生成遮挡图 $M{occ}$​，标识在对齐过程中被遮挡的区域。\n图像融合（Fusion）： # 采用 UNet（Ronneberger et al. 2015）来融合来自广角摄像头（W）的裁剪图像亮度通道与通过变形的长焦摄像头（T）参考图像（详见第 3.2 节）。\n以源图像、变形后的参考图像和遮挡mask作为输入 融合过程：\n为了保持 W 图像的颜色一致性，我们仅在亮度空间（Luminance Space）中进行融合。具体实现如下：\n输入准备： 将源图像 $I_{src}$​ 转换为灰度图像，记为 $Y_{src}$​。 将变形后的参考图像 $I~ref$转换为灰度图像，记为 $\\tilde{Y}_{ref}$​。 输入遮挡掩膜 $M_{occ}$​。 融合网络： 我们构建了一个 5 层的 UNet 网络，以 $Y_{src}$​、$\\tilde{Y}{ref}$​ 和$M{occ}$​ 作为输入，生成融合后的灰度图像 $Y_{fusion}$​。 该 UNet 的详细架构在附录中提供。 颜色恢复： 将融合得到的灰度图像 $Y_{fusion}$​ 与 $I_{src}$​ 的 UV 色彩通道相结合。 转换回 RGB 空间，生成融合输出图像 $I_{fusion}$​ 自适应混合（Adaptive Blending）： # 尽管机器学习模型在图像对齐和融合方面具有强大能力，但 W 和 T 之间的不匹配仍可能在输出中引入明显伪影。这些不匹配包括：\n景深差异（DoF differences） 像素遮挡（Occluded pixels） 对齐阶段的变形伪影（Warping artifacts） 为了解决这些问题，我们提出了一种自适应融合策略，通过结合从 失焦图、遮挡图、光流不确定性图 和 对齐拒绝图 派生的 alpha mask，自适应地将 $Y_{src}$​ 和 $Y_{fusion}$​ 融合。最终输出图像消除了显著伪影，并在 W 和 T 之间像素级一致性存在问题的情况下具有较高鲁棒性。\n长焦（T）的窄景深问题 # 我们观察到，移动设备的长焦摄像头（T）通常比广角摄像头（W）具有更窄的景深（DoF）。这是因为景深与光圈数和焦距的平方成正比.典型情况下，T 和 W 的焦距比大于 3 倍，而光圈数比小于 2.5 倍。因此，T 的景深通常显著窄于 W. 因此, 需要通过失焦图（defocus map）排除失焦像素区域，以避免伪影的引入。单幅图像的失焦图估计是一个病态问题，通常需要复杂且计算量大的机器学习模型. 为此，我们提出了一种高效算法，重新利用在对齐阶段计算的光流信息生成失焦图：\n光流信息中包含了图像的深度和运动特征，通过分析光流与场景深度的关系，可以高效生成失焦图，而无需额外的复杂模型。 失焦图（Defocus Map）生成方法 # 为了估计失焦图，我们需要确定两个关键信息：\n相机的焦点位置（focused center）：即画面中处于清晰聚焦状态的中心区域。 每个像素相对于焦点位置的相对深度（relative depth）。 由于 W 和 T 的视场角（FOV）大致是平行的（fronto-parallel），并且光流的幅值与相机视差成正比，从而与场景深度相关，我们设计了一种基于光流的高效失焦图估计算法，具体过程如下（见图 5）： 获取焦点感兴趣区域$ROI$： 从相机的自动对焦（auto-focus）模块获取焦点区域。该模块提供了一个矩形区域，表示 T 图像中大部分像素聚焦清晰的区域（即焦点 ROI）。 基于光流估计焦点位置 $x_f$​： 利用双摄像头立体视觉，将光流视为场景深度的代理。假设在静态场景中，位于同一焦平面的像素具有相似的光流向量（Szeliski 2022）。 在焦点 ROI 中对光流向量应用 k-均值聚类算法，确定聚类密度最高的区域（即最大聚类）。 聚类中心被定义为焦点位置 $x_f$。 估计像素相对深度并生成失焦图： 计算每个像素的光流向量与焦点位置 $x_f$ 的光流向量之间的欧氏距离$L2$ 距离）。 失焦图计算公式 # 失焦图 $M_{defocus}(x)$的计算公式如下： $$ M_{defocus}(x) = \\text{sigmoid} \\left( \\frac{| F_{fwd}(x) - F_{fwd}(x_f) |_2^2 - \\gamma}{\\sigma_f} \\right) $$ 其中：\n$F_{fwd}(x)$：像素 ( x ) 的正向光流向量。 $F_{fwd}(x_f)$ ：焦点位置 ( x_f ) 的正向光流向量。 $\\gamma$：控制容忍范围的阈值。 $\\sigma_f$ ：控制失焦图平滑程度的参数。 遮挡图计算公式 # 遮挡图 $M_{occ}(x)$ 的计算基于前向-后向光流一致性（Forward-Backward Flow Consistency），公式如下： $M_{occ}(x) = \\min\\left(s \\cdot | W(W(x; F_{fwd}); F_{bwd}) - x |_2, 1 \\right)$ 其中：\n$W$：双线性变形操作符（bilinear warping operator），用来根据光流将图像坐标 $x$ 映射到新的位置。 $F_{fwd}$：从源图像 $I_{src}$​ 到参考图像 $I_{ref}$​的前向光流。 $F_{bwd}$​：从参考图像 $I_{ref}$ 到源图像 $I_{src}$​ 的后向光流。 $s$：一个缩放因子，用于调整光流差异的敏感性。 $x$：源图像上的 2D 图像坐标。 光流不确定性图 # 由于稠密对应（dense correspondence）问题本质上是病态的，我们增强了 PWC-Net 的功能，使其输出光流不确定性图（flow uncertainty map），用于描述每个像素光流预测的不确定性。方法如下：\n不确定性建模：\n增强后的 PWC-Net 为每个像素预测一个多元拉普拉斯分布（multivariate Laplacian distribution）的光流向量，而非单一的点估计。它预测了两条附加通道，分别为光流在 x 和 y 方向上的对数方差（log-variance），记为 ${Var}_x$​ 和 $\\text{Var}_y$​。\n转换为像素单位：\n对数方差通过以下公式转换为像素单- 这里，S(x)S(x)S(x) 是每个像素的光流不确定性值。 $$ S(x) = \\sqrt{\\exp(\\log(\\text{Var}_x(x))) + \\exp(\\log(\\text{Var}_y(x)))} $$\n生成光流不确定性图：\n$$ M_{\\text{flow}}(x) = \\frac{\\min(S(x), s_{\\text{max}})}{s_{\\text{max}}} $$ 光流不确定性图通常在物体边界或无纹理区域（texture-less regions）中具有较高值。\n对齐拒绝图（Alignment Rejection Map） # 为了排除由于对齐错误引入的伪影，我们通过对比源图像和参考图像（对齐后）的局部相似性生成对齐拒绝图（alignment rejection map）。方法如下：\n匹配光学分辨率： 使用双线性插值调整对齐后的参考帧 $\\tilde{Y}{\\text{ref}}$​ 的分辨率，使其与 W 的光学分辨率匹配，得到下采样图像 $\\tilde{Y}{\\text{ref}}^\\downarrow$​。 计算局部差异： 对于源图像局部补丁 $P_{\\text{src}}$​ 和对齐参考图像补丁 $P_{\\tilde{\\text{ref}}}$​： 减去补丁的均值$u_{\\text{src}}$​ 和 $u_{\\text{ref}}$。 计算归一化差异： $$ P_\\delta = (P_{\\text{src}} - \\mu_{\\text{src}}) - (P_{\\tilde{\\text{ref}}} - \\mu_{\\text{ref}}) $$ 生成对齐拒绝图 $$ M_{\\text{reject}}(x) = 1 - \\exp\\left(-\\frac{|P_\\delta(x)|2^2}{\\sigma{\\text{src}}^2(x) + \\epsilon_0}\\right) $$ 最终混合 # $$ M_{\\text{blend}} = \\max(1 - M_{\\text{occ}} - M_{\\text{defocus}} - M_{\\text{flow}} - M_{\\text{reject}}, 0) $$ 最终输出图像通过 Alpha 混合生成，并裁剪回完整的 W 图像： $$ I_{\\text{final}} = \\text{uncrop}\\left(M_{\\text{blend}} \\odot I_{\\text{fusion}} + (1 - M_{\\text{blend}}) \\odot I_{\\text{src}}, W\\right) $$ 通过结合多种遮罩（如遮挡图、失焦图、光流不确定性图和对齐拒绝图），我们确保了融合结果的高质量和鲁棒性，避免了伪影、模糊或对齐错误的引入。\nImplementation Details # Results and Evaluation # Experiments: # Results # Comparisons # Critical Analysis # Strengths # Weaknesses # Insights # Broader Impact and Future Work # Broader Implications # Open Questions # Future Work # Personal Reflection # Relevance to My Work # Practical Applications # Key Learnings # Related Work and Context # Prior Work # 基于学习的单幅图像超分辨率（SISR）\n过去十年中，多种方法（例如 Christian Ledig 2017；Dong et al. 2014；Kim et al. 2016；Lai et al. 2017；Wang et al. 2018；Xu et al. 2023；Zhang et al. 2019a, 2022b, 2018）在单幅图像超分辨率领域展示了出色的效果。然而，由于该任务的严重病态特性，这些方法在较大的上采样因子（例如智能手机混合变焦常见的 2-5 倍）下会生成模糊的细节。此外，一些方法仅适用于特定领域，如人脸超分辨率（Chan et al. 2021；Gu et al. 2020；He et al. 2022；Menon et al. 2020）。\n基于互联网图像的参考超分辨率（RefSR）\nRefSR 通过低分辨率输入和一个或多个高分辨率参考图像（Pesavento et al. 2021）生成高分辨率结果。传统的 RefSR 方法假设参考图像来自互联网（Sun and Hays 2012）或在同一事件的不同时间、位置或相机型号拍摄（Wang et al. 2016；Zhang et al. 2019b），研究的重点是改进源图像与参考图像之间的密集对齐（Huang et al. 2022；Jiang et al. 2021；Xia et al. 2022；Zheng et al. 2018）或增强对不相关参考图像的鲁棒性（Lu et al. 2021；Shim et al. 2020；Xie et al. 2020；Yang et al. 2020；Zhang et al. 2019b）。\n与此不同，我们通过同步捕获 W 和 T 图像来减轻对齐的挑战，避免了由于物体运动引起的对齐问题。\n基于辅助摄像头的参考超分辨率（RefSR）\n最近的一些 RefSR 研究（Trinidad et al. 2019；Wang et al. 2021；Zhang et al. 2022a）通过辅助摄像头捕获同一场景的参考图像。然而，由于缺乏像素对齐的输入和真实标签图像对，PixelFusionNet（Trinidad et al. 2019）通过退化模型从高分辨率参考图像合成低分辨率输入，并使用像素级损失（如 l1 和 VGG 损失）进行训练。然而，这种模型在面对真实场景输入时表现不佳，主要原因是训练和推理阶段图像之间的域差异。\n另一方面，SelfDZSR（Zhang et al. 2022a）、DCSR（Wang et al. 2021）和 RefVSR（Lee et al. 2022）将参考图像视为训练或微调的目标。我们观察到，这种训练设置容易陷入退化的局部最小值：模型通常会学习恒等映射（identity mapping），仅将 T 图像的内容复制到输出中。这会导致严重的对齐错误、颜色偏移和景深不匹配等问题，这在实际摄影中是无法接受的。\n为了解决这些问题，我们在训练过程中额外捕获一张 T 图像，从而缓解上述问题并提高训练的鲁棒性。\n高效的移动设备参考超分辨率（RefSR）\n现有方法通常因使用注意力机制/Transformer（如 Wang et al. 2021；Yang et al. 2020）或深度网络架构（如 Zhang et al. 2022a）而占用大量内存。这些方法在处理 12MP 输入分辨率时，即使在拥有 40GB RAM 的 NVIDIA A100 桌面 GPU 上也可能遇到内存不足（OOM）问题，更无法在移动设备上运行。相比之下，我们的系统在移动 GPU 上处理 12MP 输入仅需 500 毫秒，使用内存仅为 300MB。\n我们的系统设计受参考图像人脸去模糊方法 [Lai et al. 2022] 的启发，但我们解决的问题更具挑战性：\n通用场景的超分辨率：我们将超分辨率应用于通用图像，而非仅专注于人脸。因此，我们的系统需要对多样化的场景更具鲁棒性，并能够处理来自两个摄像头的各种不完美和不匹配问题。 训练数据的域差异：与可以从合成数据中学习的人脸去模糊模型不同，图像超分辨率模型对训练数据的域差异更为敏感。此外，为基于参考的超分辨率任务收集真实训练数据也更加困难。因此，我们提出的自适应混合方法和双手机平台设计，成为了我们与 [Lai et al. 2022] 方法的关键区别 Historical Context # References # ","date":"15 1月 2025","externalUrl":null,"permalink":"/posts/efficient-hybird-zoom-using-camera-fusion-on-mobile-phones/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eAbstract \n    \u003cdiv id=\"abstract\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#abstract\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eDSLR 相机可以通过改变镜头距离或更换镜头类型实现多种变焦级别。然而，由于空间限制，这些技术在智能手机设备上不可行。大多数手机采用\u003cstrong\u003e混合变焦系统\u003c/strong\u003e：通常使用低变焦级别的广角（W）摄像头和高变焦级别的长焦（T）摄像头。\n为了模拟介于 W 和 T 之间的变焦级别，这些系统会对来自 W 的图像进行裁剪和数字放大，导致细节损失显著。在本文中，我们提出了一种高效的移动设备混合变焦超分辨率系统。该系统捕获\u003cstrong\u003e同步的 W 和 T 图像对\u003c/strong\u003e，并\u003cstrong\u003e利用机器学习模型将 T 的细节对齐并传递到 W\u003c/strong\u003e。我们进一步开发了一种\u003cstrong\u003e自适应混合方法\u003c/strong\u003e，该方法可以处理\u003cstrong\u003e景深不匹配、场景遮挡、流动不确定性以及对齐误差\u003c/strong\u003e。为了尽量减少域差异，我们设计了一个双手机摄像头装置，用于捕捉真实场景输入和用于监督训练的真实标签数据。在真实场景的广泛评估中，我们的方法可以在移动平台上生成 12 百万像素图像，耗时 500 毫秒，并达到了TA.\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eKey word \n    \u003cdiv id=\"key-word\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#key-word\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003ehybrid zoom, dual camera fusion, deep neural networks\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eKey Takeaways \n    \u003cdiv id=\"key-takeaways\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#key-takeaways\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eProblem Addressed \n    \u003cdiv id=\"problem-addressed\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#problem-addressed\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003e几乎所有的上采样方法（如双线性插值、双三次插值等）都会导致不同程度的画质下降, 因此, 涌现了众多利用高变焦 T 图像作为参考，为低变焦 W 图像添加真实细节的方法. 商业解决方案并未公开, 学术研究上效率较低. 。这些方法通常在移动设备上效率较低，容易受到参考图像缺陷的影响，并可能在训练和推理之间引入域差异。\n对以上问题.提出了一种混合变焦超分辨率（Hybrid Zoom Super-Resolution, HZSR）系统以加以解决。\u003c/p\u003e","title":"Efficient Hybrid Zoom using Camera Fusion on Mobile Phones Reading Notes","type":"posts"},{"content":"","date":"15 1月 2025","externalUrl":null,"permalink":"/tags/example/","section":"Tags","summary":"","title":"Example","type":"tags"},{"content":"","date":"15 1月 2025","externalUrl":null,"permalink":"/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":"","date":"15 1月 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"15 1月 2025","externalUrl":null,"permalink":"/tags/tag/","section":"Tags","summary":"","title":"Tag","type":"tags"},{"content":"an example to get you started\nThis is a heading # This is a subheading # This is a subsubheading # This is a subsubsubheading # This is a paragraph with bold and italic text. Check more at Blowfish documentation undefined\n","date":"4 1月 2025","externalUrl":null,"permalink":"/projects/1735963425218-d5-render-rhino-sync-plugin/","section":"Projects","summary":"\u003cp\u003ean example to get you started\u003c/p\u003e\n\n\n\u003ch1 class=\"relative group\"\u003eThis is a heading \n    \u003cdiv id=\"this-is-a-heading\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#this-is-a-heading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\n\n\u003ch2 class=\"relative group\"\u003eThis is a subheading \n    \u003cdiv id=\"this-is-a-subheading\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#this-is-a-subheading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eThis is a subsubheading \n    \u003cdiv id=\"this-is-a-subsubheading\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#this-is-a-subsubheading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\n\n\u003ch4 class=\"relative group\"\u003eThis is a subsubsubheading \n    \u003cdiv id=\"this-is-a-subsubsubheading\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#this-is-a-subsubsubheading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eThis is a paragraph with \u003cstrong\u003ebold\u003c/strong\u003e and \u003cem\u003eitalic\u003c/em\u003e text.\nCheck more at \u003ca href=\"https://blowfish.page/\" target=\"_blank\"\u003eBlowfish documentation\u003c/a\u003e\nundefined\u003c/p\u003e","title":"D5 Render-Rhino Sync Plugin","type":"projects"},{"content":"","date":"4 1月 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":" 自己紹介 # 私は中国山東省で生まれ、天津で育ちました。南京で7年間勉強した後、博士課程を目指して日本に来ました。\n日本のカレーとすき焼きが大好きです。趣味はサッカーと日本のロック音楽を聴くことで、特にブルーハーツとX JAPANは私の大好きなバンドです。\n経歴 # 東南大学 2016-2020 中国 ソフトウェア工学 東南大学でソフトウェア工学の学士号を取得しました。 大きな湖のある大学の図書館が気に入っています。 データ構造、コンピュータネットワーク、オペレーティングシステムなど、コンピュータサイエンスの基礎科目を学びました。 南京大学 2020-2023 中国 ソフトウェア工学 この期間、南京大学でソフトウェア工学の修士号を取得しました。主にソフトウェア工学における機械学習の応用に焦点を当てて研究を行いました。 AWS 2023-2024 中国 ソフトウェアエンジニア Amazon Web Servicesでソフトウェアエンジニアとして働き、AWS OpenSearchサービスの開発を担当しました。上海のチームメイトと職場環境がとても気に入っています。 早稲田大学 2023-2025 日本 コンピュータサイエンス 現在、早稲田大学でコンピュータサイエンスの博士課程に在籍しています。吉江修教授の下で研究を行っています。 ","externalUrl":null,"permalink":"/ja/about/","section":"かんそう","summary":"\u003ch2 class=\"relative group\"\u003e自己紹介 \n    \u003cdiv id=\"%E8%87%AA%E5%B7%B1%E7%B4%B9%E4%BB%8B\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E8%87%AA%E5%B7%B1%E7%B4%B9%E4%BB%8B\" aria-label=\"アンカー\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e私は中国山東省で生まれ、天津で育ちました。南京で7年間勉強した後、博士課程を目指して日本に来ました。\u003c/p\u003e\n\u003cp\u003e日本のカレーとすき焼きが大好きです。趣味はサッカーと日本のロック音楽を聴くことで、特にブルーハーツとX JAPANは私の大好きなバンドです。\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003e経歴 \n    \u003cdiv id=\"%E7%B5%8C%E6%AD%B4\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E7%B5%8C%E6%AD%B4\" aria-label=\"アンカー\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003col class=\"border-l-2 border-primary-500 dark:border-primary-300 list-none\"\u003e\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \n\n  \u003cspan class=\"relative block icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 640 512\"\u003e\n\u003cpath fill=\"currentColor\" d=\"M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9v28.1c0 28.4-10.8 57.7-22.3 80.8c-6.5 13-13.9 25.8-22.5 37.6C0 442.7-.9 448.3 .9 453.4s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7 .3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7C90.3 344.3 86 329.8 80 316.5V291.9c0-30.2 10.2-58.7 27.9-81.5c12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5 .8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1L624.2 182.6c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1C336.1 33.4 328.1 32 320 32zM128 408c0 35.3 86 72 192 72s192-36.7 192-72L496.7 262.6 354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6L143.3 262.6 128 408z\"/\u003e\u003c/svg\u003e\n  \u003c/span\u003e\n\n\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n        \u003ch2 class=\"mt-0\"\u003e\n          東南大学 \n        \u003c/h2\u003e\n        \n        \n        \u003ch3 class=\"\"\u003e\n          \u003cspan class=\"flex\" style=\"cursor: pointer;\"\u003e\n  \u003cspan class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    2016-2020 中国\n  \u003c/span\u003e\n\u003c/span\u003e\n        \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n      \u003ch4 class=\"mt-0\"\u003e\n        ソフトウェア工学\n      \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\n東南大学でソフトウェア工学の学士号を取得しました。\n大きな湖のある大学の図書館が気に入っています。\nデータ構造、コンピュータネットワーク、オペレーティングシステムなど、コンピュータサイエンスの基礎科目を学びました。\n\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/li\u003e\n\n\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \n\n  \u003cspan class=\"relative block icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 640 512\"\u003e\n\u003cpath fill=\"currentColor\" d=\"M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9v28.1c0 28.4-10.8 57.7-22.3 80.8c-6.5 13-13.9 25.8-22.5 37.6C0 442.7-.9 448.3 .9 453.4s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7 .3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7C90.3 344.3 86 329.8 80 316.5V291.9c0-30.2 10.2-58.7 27.9-81.5c12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5 .8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1L624.2 182.6c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1C336.1 33.4 328.1 32 320 32zM128 408c0 35.3 86 72 192 72s192-36.7 192-72L496.7 262.6 354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6L143.3 262.6 128 408z\"/\u003e\u003c/svg\u003e\n  \u003c/span\u003e\n\n\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n        \u003ch2 class=\"mt-0\"\u003e\n          南京大学 \n        \u003c/h2\u003e\n        \n        \n        \u003ch3 class=\"\"\u003e\n          \u003cspan class=\"flex\" style=\"cursor: pointer;\"\u003e\n  \u003cspan class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    2020-2023 中国\n  \u003c/span\u003e\n\u003c/span\u003e\n        \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n      \u003ch4 class=\"mt-0\"\u003e\n        ソフトウェア工学\n      \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\nこの期間、南京大学でソフトウェア工学の修士号を取得しました。主にソフトウェア工学における機械学習の応用に焦点を当てて研究を行いました。\n\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/li\u003e\n\n\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \n\n  \u003cspan class=\"relative block icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 448 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M257.2 162.7c-48.7 1.8-169.5 15.5-169.5 117.5 0 109.5 138.3 114 183.5 43.2 6.5 10.2 35.4 37.5 45.3 46.8l56.8-56S341 288.9 341 261.4V114.3C341 89 316.5 32 228.7 32 140.7 32 94 87 94 136.3l73.5 6.8c16.3-49.5 54.2-49.5 54.2-49.5 40.7-.1 35.5 29.8 35.5 69.1zm0 86.8c0 80-84.2 68-84.2 17.2 0-47.2 50.5-56.7 84.2-57.8v40.6zm136 163.5c-7.7 10-70 67-174.5 67S34.2 408.5 9.7 379c-6.8-7.7 1-11.3 5.5-8.3C88.5 415.2 203 488.5 387.7 401c7.5-3.7 13.3 2 5.5 12zm39.8 2.2c-6.5 15.8-16 26.8-21.2 31-5.5 4.5-9.5 2.7-6.5-3.8s19.3-46.5 12.7-55c-6.5-8.3-37-4.3-48-3.2-10.8 1-13 2-14-.3-2.3-5.7 21.7-15.5 37.5-17.5 15.7-1.8 41-.8 46 5.7 3.7 5.1 0 27.1-6.5 43.1z\"/\u003e\u003c/svg\u003e\n\n  \u003c/span\u003e\n\n\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n        \u003ch2 class=\"mt-0\"\u003e\n          AWS \n        \u003c/h2\u003e\n        \n        \n        \u003ch3 class=\"\"\u003e\n          \u003cspan class=\"flex\" style=\"cursor: pointer;\"\u003e\n  \u003cspan class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    2023-2024 中国\n  \u003c/span\u003e\n\u003c/span\u003e\n        \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n      \u003ch4 class=\"mt-0\"\u003e\n        ソフトウェアエンジニア\n      \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\nAmazon Web Servicesでソフトウェアエンジニアとして働き、AWS OpenSearchサービスの開発を担当しました。上海のチームメイトと職場環境がとても気に入っています。\n\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/li\u003e\n\n\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \n\n  \u003cspan class=\"relative block icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 640 512\"\u003e\n\u003cpath fill=\"currentColor\" d=\"M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9v28.1c0 28.4-10.8 57.7-22.3 80.8c-6.5 13-13.9 25.8-22.5 37.6C0 442.7-.9 448.3 .9 453.4s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7 .3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7C90.3 344.3 86 329.8 80 316.5V291.9c0-30.2 10.2-58.7 27.9-81.5c12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5 .8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1L624.2 182.6c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1C336.1 33.4 328.1 32 320 32zM128 408c0 35.3 86 72 192 72s192-36.7 192-72L496.7 262.6 354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6L143.3 262.6 128 408z\"/\u003e\u003c/svg\u003e\n  \u003c/span\u003e\n\n\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n        \u003ch2 class=\"mt-0\"\u003e\n          早稲田大学 \n        \u003c/h2\u003e\n        \n        \n        \u003ch3 class=\"\"\u003e\n          \u003cspan class=\"flex\" style=\"cursor: pointer;\"\u003e\n  \u003cspan class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    2023-2025 日本\n  \u003c/span\u003e\n\u003c/span\u003e\n        \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n      \u003ch4 class=\"mt-0\"\u003e\n        コンピュータサイエンス\n      \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\n現在、早稲田大学でコンピュータサイエンスの博士課程に在籍しています。吉江修教授の下で研究を行っています。\n\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/li\u003e\n\n\u003c/ol\u003e","title":"","type":"page"},{"content":" dwadad # ","externalUrl":null,"permalink":"/posts/test/","section":"Posts","summary":"\u003ch1 class=\"relative group\"\u003edwadad \n    \u003cdiv id=\"dwadad\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#dwadad\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e","title":"","type":"posts"},{"content":"","externalUrl":null,"permalink":"/ja/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ja/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","externalUrl":null,"permalink":"/resume/","section":"Kannsou","summary":"\u003cdiv class=\"embed-pdf-container\" id=\"embed-pdf-container-0263e339\"\u003e\n    \u003cdiv class=\"pdf-loadingWrapper\" id=\"pdf-loadingWrapper-0263e339\"\u003e\n        \u003cdiv class=\"pdf-loading\" id=\"pdf-loading-0263e339\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv id=\"overlayText\"\u003e\n      \u003ca href=\"congguan_en.pdf\" aria-label=\"Download\" download\u003e\n        \u003csvg aria-hidden=\"true\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 18 18\"\u003e\n            \u003cpath d=\"M9 13c.3 0 .5-.1.7-.3L15.4 7 14 5.6l-4 4V1H8v8.6l-4-4L2.6 7l5.7 5.7c.2.2.4.3.7.3zm-7 2h14v2H2z\" /\u003e\n        \u003c/svg\u003e\n      \u003c/a\u003e\n    \u003c/div\u003e\n    \u003ccanvas class=\"pdf-canvas\" id=\"pdf-canvas-0263e339\"\u003e\u003c/canvas\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"pdf-paginator\" id=\"pdf-paginator-0263e339\"\u003e\n    \u003cbutton id=\"pdf-prev-0263e339\"\u003ePrevious\u003c/button\u003e\n    \u003cbutton id=\"pdf-next-0263e339\"\u003eNext\u003c/button\u003e \u0026nbsp; \u0026nbsp;\n    \u003cspan\u003e\n      \u003cspan class=\"pdf-pagenum\" id=\"pdf-pagenum-0263e339\"\u003e\u003c/span\u003e / \u003cspan class=\"pdf-pagecount\" id=\"pdf-pagecount-0263e339\"\u003e\u003c/span\u003e\n    \u003c/span\u003e\n    \u003ca class=\"pdf-source\" id=\"pdf-source-0263e339\" href=\"congguan_en.pdf\"\u003e[pdf]\u003c/a\u003e\n\u003c/div\u003e\n\n\u003cnoscript\u003e\nView the PDF file \u003ca class=\"pdf-source\" id=\"pdf-source-noscript-0263e339\" href=\"congguan_en.pdf\"\u003ehere\u003c/a\u003e.\n\u003c/noscript\u003e\n\n\u003cscript type=\"text/javascript\"\u003e\n    (function(){\n    var url = 'congguan_en.pdf';\n\n    var hidePaginator = \"\" === \"true\";\n    var hideLoader = \"\" === \"true\";\n    var selectedPageNum = parseInt(\"\") || 1;\n\n    \n    var pdfjsLib = window['pdfjs-dist/build/pdf'];\n\n    \n    if (pdfjsLib.GlobalWorkerOptions.workerSrc == '')\n      pdfjsLib.GlobalWorkerOptions.workerSrc = \"https:\\/\\/teiboku.github.io\\/\" + '/js/pdf-js/build/pdf.worker.js';\n\n    \n    var pdfDoc = null,\n        pageNum = selectedPageNum,\n        pageRendering = false,\n        pageNumPending = null,\n        scale = 3,\n        canvas = document.getElementById('pdf-canvas-0263e339'),\n        ctx = canvas.getContext('2d'),\n        paginator = document.getElementById(\"pdf-paginator-0263e339\"),\n        loadingWrapper = document.getElementById('pdf-loadingWrapper-0263e339');\n\n\n    \n    showPaginator();\n    showLoader();\n\n    \n\n    function renderPage(num) {\n      pageRendering = true;\n      \n      pdfDoc.getPage(num).then(function(page) {\n        var viewport = page.getViewport({scale: scale});\n        canvas.height = viewport.height;\n        canvas.width = viewport.width;\n\n        \n        var renderContext = {\n          canvasContext: ctx,\n          viewport: viewport\n        };\n        var renderTask = page.render(renderContext);\n\n        \n        renderTask.promise.then(function() {\n          pageRendering = false;\n          showContent();\n\n          if (pageNumPending !== null) {\n            \n            renderPage(pageNumPending);\n            pageNumPending = null;\n          }\n        });\n      });\n\n      \n      document.getElementById('pdf-pagenum-0263e339').textContent = num;\n    }\n\n    \n\n    function showContent() {\n      loadingWrapper.style.display = 'none';\n      canvas.style.display = 'block';\n    }\n\n    \n\n    function showLoader() {\n      if(hideLoader) return\n      loadingWrapper.style.display = 'flex';\n      canvas.style.display = 'none';\n    }\n\n    \n\n    function showPaginator() {\n      if(hidePaginator) return\n      paginator.style.display = 'block';\n    }\n\n    \n\n    function queueRenderPage(num) {\n      if (pageRendering) {\n        pageNumPending = num;\n      } else {\n        renderPage(num);\n      }\n    }\n\n    \n\n    function onPrevPage() {\n      if (pageNum \u003c= 1) {\n        return;\n      }\n      pageNum--;\n      queueRenderPage(pageNum);\n    }\n    document.getElementById('pdf-prev-0263e339').addEventListener('click', onPrevPage);\n\n    \n\n    function onNextPage() {\n      if (pageNum \u003e= pdfDoc.numPages) {\n        return;\n      }\n      pageNum++;\n      queueRenderPage(pageNum);\n    }\n    document.getElementById('pdf-next-0263e339').addEventListener('click', onNextPage);\n\n    \n\n    pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {\n      pdfDoc = pdfDoc_;\n      var numPages = pdfDoc.numPages;\n      document.getElementById('pdf-pagecount-0263e339').textContent = numPages;\n\n      \n      if(pageNum \u003e numPages) {\n        pageNum = numPages\n      }\n\n      \n      renderPage(pageNum);\n    });\n    })();\n\u003c/script\u003e","title":"My Resume","type":"page"},{"content":"","externalUrl":null,"permalink":"/ja/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/ja/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","externalUrl":null,"permalink":"/ja/","section":"かんそう","summary":"","title":"かんそう","type":"page"}]